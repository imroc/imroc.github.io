<!DOCTYPE html>
<html lang="zh">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title> Kubernetes 疑难杂症排查分享：神秘的溢出与丢包 - roc的博客|Cloud Native|Kubernetes|Go|Golang|Service Mesh</title>
  <meta name="description" content="roc的博客|Cloud Native|Kubernetes|Go|Golang|Service Mesh" />
  <meta property="og:title" content="Kubernetes 疑难杂症排查分享：神秘的溢出与丢包" />
  <meta name="twitter:title" content="Kubernetes 疑难杂症排查分享：神秘的溢出与丢包" />
  <meta name="description" content="
上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。

问题描述
有用户反馈大量图片加载不出来。
图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client &ndash;&gt; k8s ingress &ndash;&gt; nginx &ndash;&gt; nfs。
猜测
猜测: ingress 图片下载路径对应的后端服务出问题了。
验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！
抓包
继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：
# 拿到 pod 中 nginx 的容器 id
$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 &#34;^Containers:&#34; | grep -Eo &#39;docker://.*$&#39; | head -n 1 | sed &#39;s/docker:\/\/\(.*\)$/\1/&#39;
49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e

# 通过容器 id 拿到 nginx 进程 pid
$ docker inspect -f {{.State.Pid}} 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e
3985

# 进入 nginx 进程所在的 netns
$ nsenter -n -t 3985

# 查看容器 netns 中的网卡信息，确认下
$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.26.0.8/26 scope global eth0
       valid_lft forever preferred_lft forever
使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:
tcpdump -i eth0 -nnnn -ttt port 24568
在其它节点准备使用 nc 指定源端口为 24568 向容器发包：
nc -u 24568 172.16.1.21 80
观察抓包结果：
00:00:00.000000 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000206334 ecr 0,nop,wscale 9], length 0
00:00:01.032218 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000207366 ecr 0,nop,wscale 9], length 0
00:00:02.011962 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000209378 ecr 0,nop,wscale 9], length 0
00:00:04.127943 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000213506 ecr 0,nop,wscale 9], length 0
00:00:08.192056 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000221698 ecr 0,nop,wscale 9], length 0
00:00:16.127983 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000237826 ecr 0,nop,wscale 9], length 0
00:00:33.791988 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000271618 ecr 0,nop,wscale 9], length 0
SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。
排除是 iptables 规则问题，在容器 netns 中使用 netstat -s 检查下是否有丢包统计:
$ netstat -s | grep -E &#39;overflow|drop&#39;
    12178939 times the listen queue of a socket overflowed
    12247395 SYNs to LISTEN sockets dropped
果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。">
  <meta property="og:description" content="
上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。

问题描述
有用户反馈大量图片加载不出来。
图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client &ndash;&gt; k8s ingress &ndash;&gt; nginx &ndash;&gt; nfs。
猜测
猜测: ingress 图片下载路径对应的后端服务出问题了。
验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！
抓包
继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：
# 拿到 pod 中 nginx 的容器 id
$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 &#34;^Containers:&#34; | grep -Eo &#39;docker://.*$&#39; | head -n 1 | sed &#39;s/docker:\/\/\(.*\)$/\1/&#39;
49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e

# 通过容器 id 拿到 nginx 进程 pid
$ docker inspect -f {{.State.Pid}} 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e
3985

# 进入 nginx 进程所在的 netns
$ nsenter -n -t 3985

# 查看容器 netns 中的网卡信息，确认下
$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.26.0.8/26 scope global eth0
       valid_lft forever preferred_lft forever
使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:
tcpdump -i eth0 -nnnn -ttt port 24568
在其它节点准备使用 nc 指定源端口为 24568 向容器发包：
nc -u 24568 172.16.1.21 80
观察抓包结果：
00:00:00.000000 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000206334 ecr 0,nop,wscale 9], length 0
00:00:01.032218 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000207366 ecr 0,nop,wscale 9], length 0
00:00:02.011962 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000209378 ecr 0,nop,wscale 9], length 0
00:00:04.127943 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000213506 ecr 0,nop,wscale 9], length 0
00:00:08.192056 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000221698 ecr 0,nop,wscale 9], length 0
00:00:16.127983 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000237826 ecr 0,nop,wscale 9], length 0
00:00:33.791988 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000271618 ecr 0,nop,wscale 9], length 0
SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。
排除是 iptables 规则问题，在容器 netns 中使用 netstat -s 检查下是否有丢包统计:
$ netstat -s | grep -E &#39;overflow|drop&#39;
    12178939 times the listen queue of a socket overflowed
    12247395 SYNs to LISTEN sockets dropped
果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。">
  <meta name="twitter:description" content="
上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。

问题描述
有用户反馈大量图片加载不出来。
图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文 …">
  <meta name="author" content="{Description { .Site.Author.name }}"/>
  <link href='https://imroc.io/favicon.png' rel='icon' type='image/x-icon'/>
  <meta property="og:image" content="https://res.cloudinary.com/imroc/image/upload/v1521031841/avatar.png" />
  <meta name="twitter:image" content="https://res.cloudinary.com/imroc/image/upload/v1521031841/avatar.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@imrocchan" />
  <meta name="twitter:creator" content="@imrocchan" />
  <meta property="og:url" content="https://imroc.io/posts/kubernetes-overflow-and-drop/" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="roc" />

  <meta name="generator" content="Hugo 0.65.3" />
  <link rel="canonical" href="https://imroc.io/posts/kubernetes-overflow-and-drop/" />
  <link rel="alternate" href="https://imroc.io/index.xml" type="application/rss+xml" title="roc"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css"
    integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/5.5.0/css/fontawesome.min.css"
    integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.bootcss.com/twitter-bootstrap/3.3.7/css/bootstrap.min.css"
    integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

  






<link rel="stylesheet" href='/css/bundle.min.8fad4c9e81931afed0cc9eee450df2ae0e60d1129a4cd7ee011629c97c6394e9.css' integrity='sha256-j61MnoGTGv7QzJ7uRQ3yrg5g0RKaTNfuARYpyXxjlOk='><script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?05e1e8b7484a08c51cd0953664168cd7";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">切换导航</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://imroc.io/">roc</a>
    </div>
    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" href="javascript:void(0)">分类</a>
              <div class="navlinks-children">
                
                
                  <a href="https://imroc.io/categories/kubernetes">kubernetes</a>
                
                
                  <a href="https://imroc.io/categories/docker">docker</a>
                
                
                  <a href="https://imroc.io/categories/golang">golang</a>
                
                
                  <a href="https://imroc.io/categories/geek">geek</a>
                
                
                  <a href="https://imroc.io/categories/istio">istio</a>
                
              </div>
            </li>
          
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" href="javascript:void(0)">书籍</a>
              <div class="navlinks-children">
                
                
                  <a href="https://k8s.imroc.io">Kubernetes 实践指南</a>
                
                
                  <a href="https://istio.imroc.io">Istio 实践指南</a>
                
                
                  <a href="https://book.kubetencent.io">腾讯云容器服务指南</a>
                
              </div>
            </li>
          
        

        
          
            <li>
              
                
              
                
                  <a href="/en" lang="en">English</a>
                
              
            </li>
          
        

        
        <li>
          <a href="#modalSearch" data-toggle="modal" data-target="#modalSearch" style="outline: none;">
            <span id="searchGlyph" class="glyphicon glyphicon-search"></span>
          </a>
        </li>
        
      </ul>
    </div>
    <div class="avatar-container">
      <div class="avatar-img-border">
        
          <a title="roc" href="https://imroc.io/">
            <img class="avatar-img" src="https://res.cloudinary.com/imroc/image/upload/v1521031841/avatar.png" alt="roc" />
          </a>
        
      </div>
    </div>
  </div>
</nav>


  <div id="modalSearch" class="modal fade" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal">&times;</button>
          <h4 class="modal-title">搜索</h4>
        </div>
        <div class="modal-body">
            
<div class="aa-input-container" id="aa-input-container">
    <input type="search" id="aa-search-input" class="aa-input-search" placeholder="Search for titles or URIs..."
        name="search" autocomplete="off" />
    <svg class="aa-input-icon" viewBox="654 -372 1664 1664">
        <path
            d="M1806,332c0-123.3-43.8-228.8-131.5-316.5C1586.8-72.2,1481.3-116,1358-116s-228.8,43.8-316.5,131.5  C953.8,103.2,910,208.7,910,332s43.8,228.8,131.5,316.5C1129.2,736.2,1234.7,780,1358,780s228.8-43.8,316.5-131.5  C1762.2,560.8,1806,455.3,1806,332z M2318,1164c0,34.7-12.7,64.7-38,90s-55.3,38-90,38c-36,0-66-12.7-90-38l-343-342  c-119.3,82.7-252.3,124-399,124c-95.3,0-186.5-18.5-273.5-55.5s-162-87-225-150s-113-138-150-225S654,427.3,654,332  s18.5-186.5,55.5-273.5s87-162,150-225s138-113,225-150S1262.7-372,1358-372s186.5,18.5,273.5,55.5s162,87,225,150s113,138,150,225  S2062,236.7,2062,332c0,146.7-41.3,279.7-124,399l343,343C2305.7,1098.7,2318,1128.7,2318,1164z" />
    </svg>
</div>

<script src="https://res.cloudinary.com/jimmysong/raw/upload/rootsongjc-hugo/algoliasearch.min.js"></script>
<script src="https://res.cloudinary.com/jimmysong/raw/upload/rootsongjc-hugo/autocomplete.min.js"></script>
<script>
    var client = algoliasearch("B6Q6PSGUV5", "5b4678e9995fe4211c4fa43ad2ffdab5");
    var index = client.initIndex("imroc-blog");
    
    autocomplete('#aa-search-input', {
        hint: false
    }, {
        source: autocomplete.sources.hits(index, {
            hitsPerPage: 5
        }),
        
        displayKey: 'name',
        
        templates: {
            
            suggestion: function (suggestion) {
                return '<span>' + '<a href="/' + suggestion.uri + '">' + suggestion._highlightResult.title.value +
                        '</a></span>';
            }
        }
    });
</script>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">close</button>
        </div>
      </div>
    </div>
  </div>

    
  
  
  




  
    <div id="header-big-imgs" data-num-img=1 data-img-src-1="https://res.cloudinary.com/imroc/image/upload/v1515849754/blog/banner/hacker.jpg" data-img-desc-1="Hacker"></div>
  

  <header class="header-section has-img">
    
      <div class="intro-header big-img">
        
        
        <div class="container">
          <div class="row">
              <div class="col-lg-12 col-md-12 col-md-offset-0">
                
                <div class="post-heading">
                
                  
                     <h1>Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</h1>
                     
                    <span class="post-meta">
  
  发表于 2020-01-12
  
  
</span>


                    
                  
                  
              </div>
            </div>
          </div>
        </div>
        <span class="img-desc" style="display: inline;"></span>
      </div>
    
    <div class="intro-header no-img">
      
      <div class="container">
        <div class="row">
          <div class="col-lg-12 col-md-12 col-md-offset-0">
            <div class="posts-heading">
                <h1 align="center">Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</h1>
                
                
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main" itemscope itemtype="http://schema.org/Article">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            
            <div>
                
                <h5 id="tags" style="margin-top: 30px;">标签: 
                  
                      <a
                        href='https://imroc.io/tags/kubernetes/'>kubernetes</a>
                      &nbsp;
                  
                </h5>
            </div>
            
              
            <article role="main" class="blog-post" itemprop="articleBody" id="content">
                
<aside class="toc">
  <nav id="TableOfContents">
  <ul>
    <li><a href="#问题描述">问题描述</a></li>
    <li><a href="#猜测">猜测</a></li>
    <li><a href="#抓包">抓包</a></li>
    <li><a href="#syn-queue-与-accept-queue">syn queue 与 accept queue</a></li>
    <li><a href="#listen-与-accept">listen 与 accept</a></li>
    <li><a href="#linux-的-backlog">Linux 的 backlog</a></li>
    <li><a href="#队列溢出">队列溢出</a></li>
    <li><a href="#回到问题上来">回到问题上来</a></li>
    <li><a href="#somaxconn-的默认值很小">somaxconn 的默认值很小</a>
      <ul>
        <li><a href="#方式一-使用-k8s-sysctls-特性直接给-pod-指定内核参数">方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数</a></li>
        <li><a href="#方式二-使用-initcontainers-设置内核参数">方式二: 使用 initContainers 设置内核参数</a></li>
        <li><a href="#方式三-安装-tuning-cni-插件统一设置-sysctl">方式三: 安装 tuning CNI 插件统一设置 sysctl</a></li>
      </ul>
    </li>
    <li><a href="#nginx-的-backlog">nginx 的 backlog</a></li>
    <li><a href="#参考资料">参考资料</a></li>
  </ul>
</nav>
</aside>


                <blockquote>
<p>上一篇 <a href="https://imroc.io/posts/kubernetes/kubernetes-no-route-to-host/">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a> 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。</p>
</blockquote>
<h2 id="问题描述">问题描述</h2>
<p>有用户反馈大量图片加载不出来。</p>
<p>图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client &ndash;&gt; k8s ingress &ndash;&gt; nginx &ndash;&gt; nfs。</p>
<h2 id="猜测">猜测</h2>
<p>猜测: ingress 图片下载路径对应的后端服务出问题了。</p>
<p>验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！</p>
<h2 id="抓包">抓包</h2>
<p>继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># 拿到 pod 中 nginx 的容器 id</span>
$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 <span style="color:#e6db74">&#34;^Containers:&#34;</span> | grep -Eo <span style="color:#e6db74">&#39;docker://.*$&#39;</span> | head -n <span style="color:#ae81ff">1</span> | sed <span style="color:#e6db74">&#39;s/docker:\/\/\(.*\)$/\1/&#39;</span>
49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e

<span style="color:#75715e"># 通过容器 id 拿到 nginx 进程 pid</span>
$ docker inspect -f <span style="color:#f92672">{{</span>.State.Pid<span style="color:#f92672">}}</span> 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e
<span style="color:#ae81ff">3985</span>

<span style="color:#75715e"># 进入 nginx 进程所在的 netns</span>
$ nsenter -n -t <span style="color:#ae81ff">3985</span>

<span style="color:#75715e"># 查看容器 netns 中的网卡信息，确认下</span>
$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen <span style="color:#ae81ff">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noqueue state UP group default
    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid <span style="color:#ae81ff">0</span>
    inet 172.26.0.8/26 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre></div><p>使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tcpdump -i eth0 -nnnn -ttt port <span style="color:#ae81ff">24568</span>
</code></pre></div><p>在其它节点准备使用 nc 指定源端口为 24568 向容器发包：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nc -u <span style="color:#ae81ff">24568</span> 172.16.1.21 <span style="color:#ae81ff">80</span>
</code></pre></div><p>观察抓包结果：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">00:00:00.000000 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags <span style="color:#f92672">[</span>S<span style="color:#f92672">]</span>, seq 416500297, win 29200, options <span style="color:#f92672">[</span>mss 1424,sackOK,TS val <span style="color:#ae81ff">3000206334</span> ecr 0,nop,wscale 9<span style="color:#f92672">]</span>, length <span style="color:#ae81ff">0</span>
00:00:01.032218 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags <span style="color:#f92672">[</span>S<span style="color:#f92672">]</span>, seq 416500297, win 29200, options <span style="color:#f92672">[</span>mss 1424,sackOK,TS val <span style="color:#ae81ff">3000207366</span> ecr 0,nop,wscale 9<span style="color:#f92672">]</span>, length <span style="color:#ae81ff">0</span>
00:00:02.011962 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags <span style="color:#f92672">[</span>S<span style="color:#f92672">]</span>, seq 416500297, win 29200, options <span style="color:#f92672">[</span>mss 1424,sackOK,TS val <span style="color:#ae81ff">3000209378</span> ecr 0,nop,wscale 9<span style="color:#f92672">]</span>, length <span style="color:#ae81ff">0</span>
00:00:04.127943 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags <span style="color:#f92672">[</span>S<span style="color:#f92672">]</span>, seq 416500297, win 29200, options <span style="color:#f92672">[</span>mss 1424,sackOK,TS val <span style="color:#ae81ff">3000213506</span> ecr 0,nop,wscale 9<span style="color:#f92672">]</span>, length <span style="color:#ae81ff">0</span>
00:00:08.192056 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags <span style="color:#f92672">[</span>S<span style="color:#f92672">]</span>, seq 416500297, win 29200, options <span style="color:#f92672">[</span>mss 1424,sackOK,TS val <span style="color:#ae81ff">3000221698</span> ecr 0,nop,wscale 9<span style="color:#f92672">]</span>, length <span style="color:#ae81ff">0</span>
00:00:16.127983 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags <span style="color:#f92672">[</span>S<span style="color:#f92672">]</span>, seq 416500297, win 29200, options <span style="color:#f92672">[</span>mss 1424,sackOK,TS val <span style="color:#ae81ff">3000237826</span> ecr 0,nop,wscale 9<span style="color:#f92672">]</span>, length <span style="color:#ae81ff">0</span>
00:00:33.791988 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags <span style="color:#f92672">[</span>S<span style="color:#f92672">]</span>, seq 416500297, win 29200, options <span style="color:#f92672">[</span>mss 1424,sackOK,TS val <span style="color:#ae81ff">3000271618</span> ecr 0,nop,wscale 9<span style="color:#f92672">]</span>, length <span style="color:#ae81ff">0</span>
</code></pre></div><p>SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。</p>
<p>排除是 iptables 规则问题，在容器 netns 中使用 <code>netstat -s</code> 检查下是否有丢包统计:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ netstat -s | grep -E <span style="color:#e6db74">&#39;overflow|drop&#39;</span>
    <span style="color:#ae81ff">12178939</span> times the listen queue of a socket overflowed
    <span style="color:#ae81ff">12247395</span> SYNs to LISTEN sockets dropped
</code></pre></div><p>果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。</p>
<h2 id="syn-queue-与-accept-queue">syn queue 与 accept queue</h2>
<p>Linux 进程监听端口时，内核会给它对应的 socket 分配两个队列：</p>
<ul>
<li>syn queue: 半连接队列。server 收到 SYN 后，连接会先进入 <code>SYN_RCVD</code> 状态，并放入 syn queue，此队列的包对应还没有完全建立好的连接（TCP 三次握手还没完成）。</li>
<li>accept queue: 全连接队列。当 TCP 三次握手完成之后，连接会进入 <code>ESTABELISHED</code> 状态并从 syn queue 移到 accept queue，等待被进程调用 <code>accept()</code> 系统调用 &ldquo;拿走&rdquo;。</li>
</ul>
<blockquote>
<p>注意：这两个队列的连接都还没有真正被应用层接收到，当进程调用 <code>accept()</code> 后，连接才会被应用层处理，具体到我们这个问题的场景就是 nginx 处理 HTTP 请求。</p>
</blockquote>
<p>为了更好理解，可以看下这张 TCP 连接建立过程的示意图：</p>
<p><img src="https://imroc.io/assets/blog/troubleshooting-k8s-network/backlog.png" alt=""></p>
<h2 id="listen-与-accept">listen 与 accept</h2>
<p>不管使用什么语言和框架，在写 server 端应用时，它们的底层在监听端口时最终都会调用 <code>listen()</code> 系统调用，处理新请求时都会先调用 <code>accept()</code> 系统调用来获取新的连接，然后再处理请求，只是有各自不同的封装而已，以 go 语言为例：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#75715e">// 调用 listen 监听端口
</span><span style="color:#75715e"></span><span style="color:#a6e22e">l</span>, <span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">net</span>.<span style="color:#a6e22e">Listen</span>(<span style="color:#e6db74">&#34;tcp&#34;</span>, <span style="color:#e6db74">&#34;:80&#34;</span>)
<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
	panic(<span style="color:#a6e22e">err</span>)
}
<span style="color:#66d9ef">for</span> {
	<span style="color:#75715e">// 不断调用 accept 获取新连接，如果 accept queue 为空就一直阻塞
</span><span style="color:#75715e"></span>	<span style="color:#a6e22e">conn</span>, <span style="color:#a6e22e">err</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">l</span>.<span style="color:#a6e22e">Accept</span>()
	<span style="color:#66d9ef">if</span> <span style="color:#a6e22e">err</span> <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nil</span> {
		<span style="color:#a6e22e">log</span>.<span style="color:#a6e22e">Println</span>(<span style="color:#e6db74">&#34;accept error:&#34;</span>, <span style="color:#a6e22e">err</span>)
		<span style="color:#66d9ef">continue</span>
    }
	<span style="color:#75715e">// 每来一个新连接意味着一个新请求，启动协程处理请求
</span><span style="color:#75715e"></span>	<span style="color:#66d9ef">go</span> <span style="color:#a6e22e">handle</span>(<span style="color:#a6e22e">conn</span>)
}
</code></pre></div><h2 id="linux-的-backlog">Linux 的 backlog</h2>
<p>内核既然给监听端口的 socket 分配了 syn queue 与 accept queue 两个队列，那它们有大小限制吗？可以无限往里面塞数据吗？当然不行！ 资源是有限的，尤其是在内核态，所以需要限制一下这两个队列的大小。那么它们的大小是如何确定的呢？我们先来看下 listen 这个系统调用:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">int listen<span style="color:#f92672">(</span>int sockfd, int backlog<span style="color:#f92672">)</span>
</code></pre></div><p>可以看到，能够传入一个整数类型的 <code>backlog</code> 参数，我们再通过 <code>man listen</code> 看下解释：</p>
<p><code>The behavior of the backlog argument on TCP sockets changed with Linux 2.2.  Now it specifies the queue length for completely established sockets waiting to  be  accepted,  instead  of  the  number  of  incomplete  connection requests.   The  maximum  length  of  the queue for incomplete sockets can be set using /proc/sys/net/ipv4/tcp_max_syn_backlog.  When syncookies are enabled there is no logical maximum length and this setting is ignored.  See tcp(7) for more information. </code></p>
<p><code>If the backlog argument is greater than the value in /proc/sys/net/core/somaxconn, then it is silently truncated to that value; the default value in this file is 128.  In kernels before 2.4.25, this limit  was  a  hard  coded value, SOMAXCONN, with the value 128.</code></p>
<p>继续深挖了一下源码，结合这里的解释提炼一下：</p>
<ul>
<li>listen 的 backlog 参数同时指定了 socket 的 syn queue 与 accept queue 大小。</li>
<li>accept queue 最大不能超过 <code>net.core.somaxconn</code> 的值，即:
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">max accept queue size = min(backlog, net.core.somaxconn)
</code></pre></div></li>
<li>如果启用了 syncookies (net.ipv4.tcp_syncookies=1)，当 syn queue 满了，server 还是可以继续接收 <code>SYN</code> 包并回复 <code>SYN+ACK</code> 给 client，只是不会存入 syn queue 了。因为会利用一套巧妙的 syncookies 算法机制生成隐藏信息写入响应的 <code>SYN+ACK</code> 包中，等 client 回 <code>ACK</code> 时，server 再利用 syncookies 算法校验报文，校验通过后三次握手就顺利完成了。所以如果启用了 syncookies，syn queue 的逻辑大小是没有限制的，</li>
<li>syncookies 通常都是启用了的，所以一般不用担心 syn queue 满了导致丢包。syncookies 是为了防止 SYN Flood 攻击 (一种常见的 DDoS 方式)，攻击原理就是 client 不断发 SYN 包但不回最后的 ACK，填满 server 的 syn queue 从而无法建立新连接，导致 server 拒绝服务。</li>
<li>如果 syncookies 没有启用，syn queue 的大小就有限制，除了跟 accept queue 一样受 <code>net.core.somaxconn</code> 大小限制之外，还会受到 <code>net.ipv4.tcp_max_syn_backlog</code> 的限制，即:
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">max syn queue size = min(backlog, net.core.somaxconn, net.ipv4.tcp_max_syn_backlog)
</code></pre></div></li>
</ul>
<p>4.3 及其之前版本的内核，syn queue 的大小计算方式跟现在新版内核这里还不一样，详细请参考 commit <a href="https://github.com/torvalds/linux/commit/ef547f2ac16bd9d77a780a0e7c70857e69e8f23f#diff-56ecfd3cd70d57cde321f395f0d8d743L43">ef547f2ac16b</a></p>
<h2 id="队列溢出">队列溢出</h2>
<p>毫无疑问，在队列大小有限制的情况下，如果队列满了，再有新连接过来肯定就有问题。</p>
<p>翻下 linux 源码，看下处理 SYN 包的部分，在 <code>net/ipv4/tcp_input.c</code> 的 <code>tcp_conn_request</code> 函数:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">if</span> ((net<span style="color:#f92672">-&gt;</span>ipv4.sysctl_tcp_syncookies <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">||</span>
     inet_csk_reqsk_queue_is_full(sk)) <span style="color:#f92672">&amp;&amp;</span> <span style="color:#f92672">!</span>isn) {
	want_cookie <span style="color:#f92672">=</span> tcp_syn_flood_action(sk, rsk_ops<span style="color:#f92672">-&gt;</span>slab_name);
	<span style="color:#66d9ef">if</span> (<span style="color:#f92672">!</span>want_cookie)
		<span style="color:#66d9ef">goto</span> drop;
}

<span style="color:#66d9ef">if</span> (sk_acceptq_is_full(sk)) {
	NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
	<span style="color:#66d9ef">goto</span> drop;
}
</code></pre></div><p><code>goto drop</code> 最终会走到 <code>tcp_listendrop</code> 函数，实际上就是将 <code>ListenDrops</code> 计数器 +1:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">static</span> <span style="color:#66d9ef">inline</span> <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">tcp_listendrop</span>(<span style="color:#66d9ef">const</span> <span style="color:#66d9ef">struct</span> sock <span style="color:#f92672">*</span>sk)
{
	atomic_inc(<span style="color:#f92672">&amp;</span>((<span style="color:#66d9ef">struct</span> sock <span style="color:#f92672">*</span>)sk)<span style="color:#f92672">-&gt;</span>sk_drops);
	__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENDROPS);
}
</code></pre></div><p>大致可以看出来，对于 SYN 包：</p>
<ul>
<li>如果 syn queue 满了并且没有开启 syncookies 就丢包，并将 <code>ListenDrops</code> 计数器 +1。</li>
<li>如果 accept queue 满了也会丢包，并将 <code>ListenOverflows</code> 和 <code>ListenDrops</code> 计数器 +1。</li>
</ul>
<p>而我们前面排查问题通过 <code>netstat -s</code> 看到的丢包统计，其实就是对应的 <code>ListenOverflows</code> 和 <code>ListenDrops</code> 这两个计数器。</p>
<p>除了用 <code>netstat -s</code>，还可以使用 <code>nstat -az</code> 直接看系统内各个计数器的值:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ nstat -az | grep -E <span style="color:#e6db74">&#39;TcpExtListenOverflows|TcpExtListenDrops&#39;</span>
TcpExtListenOverflows           <span style="color:#ae81ff">12178939</span>              0.0
TcpExtListenDrops               <span style="color:#ae81ff">12247395</span>              0.0
</code></pre></div><p>另外，对于低版本内核，当 accept queue 满了，并不会完全丢弃 SYN 包，而是对 SYN 限速。把内核源码切到 3.10 版本，看 <code>net/ipv4/tcp_ipv4.c</code> 中 <code>tcp_v4_conn_request</code> 函数:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#75715e">/* Accept backlog is full. If we have already queued enough
</span><span style="color:#75715e"> * of warm entries in syn queue, drop request. It is better than
</span><span style="color:#75715e"> * clogging syn queue with openreqs with exponentially increasing
</span><span style="color:#75715e"> * timeout.
</span><span style="color:#75715e"> */</span>
<span style="color:#66d9ef">if</span> (sk_acceptq_is_full(sk) <span style="color:#f92672">&amp;&amp;</span> inet_csk_reqsk_queue_young(sk) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>) {
        NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
        <span style="color:#66d9ef">goto</span> drop;
}
</code></pre></div><p>其中 <code>inet_csk_reqsk_queue_young(sk) &gt; 1</code> 的条件实际就是用于限速，仿佛在对 client 说: 哥们，你慢点！我的 accept queue 都满了，即便咱们握手成功，连接也可能放不进去呀。</p>
<h2 id="回到问题上来">回到问题上来</h2>
<p>总结之前观察到两个现象：</p>
<ul>
<li>容器内抓包发现收到 client 的 SYN，但 nginx 没回包。</li>
<li>通过 <code>netstat -s</code> 发现有溢出和丢包的统计 (<code>ListenOverflows</code> 与 <code>ListenDrops</code>)。</li>
</ul>
<p>根据之前的分析，我们可以推测是 syn queue 或 accept queue 满了。</p>
<p>先检查下 syncookies 配置:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cat /proc/sys/net/ipv4/tcp_syncookies
<span style="color:#ae81ff">1</span>
</code></pre></div><p>确认启用了 <code>syncookies</code>，所以 syn queue 大小没有限制，不会因为 syn queue 满而丢包，并且即便没开启 <code>syncookies</code>，syn queue 有大小限制，队列满了也不会使 <code>ListenOverflows</code> 计数器 +1。</p>
<p>从计数器结果来看，<code>ListenOverflows</code> 和 <code>ListenDrops</code> 的值差别不大，所以推测很有可能是 accept queue 满了，因为当 accept queue 满了会丢 SYN 包，并且同时将 <code>ListenOverflows</code> 与 <code>ListenDrops</code> 计数器分别 +1。</p>
<p>如何验证 accept queue 满了呢？可以在容器的 netns 中执行 <code>ss -lnt</code> 看下:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ss -lnt
State      Recv-Q Send-Q Local Address:Port                Peer Address:Port
LISTEN     <span style="color:#ae81ff">129</span>    <span style="color:#ae81ff">128</span>                *:80                             *:*
</code></pre></div><p>通过这条命令我们可以看到当前 netns 中监听 tcp 80 端口的 socket，<code>Send-Q</code> 为 128，<code>Recv-Q</code> 为 129。</p>
<p>什么意思呢？通过调研得知：</p>
<ul>
<li>对于 <code>LISTEN</code> 状态，<code>Send-Q</code> 表示 accept queue 的最大限制大小，<code>Recv-Q</code> 表示其实际大小。</li>
<li>对于 <code>ESTABELISHED</code> 状态，<code>Send-Q</code> 和 <code>Recv-Q</code> 分别表示发送和接收数据包的 buffer。</li>
</ul>
<p>所以，看这里输出结果可以得知 accept queue 满了，当 <code>Recv-Q</code> 的值比 <code>Send-Q</code> 大 1 时表明 accept queue 溢出了，如果再收到 SYN 包就会丢弃掉。</p>
<p>导致 accept queue 满的原因一般都是因为进程调用 <code>accept()</code> 太慢了，导致大量连接不能被及时 &ldquo;拿走&rdquo;。</p>
<p>那么什么情况下进程调用 <code>accept()</code> 会很慢呢？猜测可能是进程连接负载高，处理不过来。</p>
<p>而负载高不仅可能是 CPU 繁忙导致，还可能是 IO 慢导致，当文件 IO 慢时就会有很多 IO WAIT，在 IO WAIT 时虽然 CPU 不怎么干活，但也会占据 CPU 时间片，影响 CPU 干其它活。</p>
<p>最终进一步定位发现是 nginx pod 挂载的 nfs 服务对应的 nfs server 负载较高，导致 IO 延时较大，从而使 nginx 调用 <code>accept()</code> 变慢，accept queue 溢出，使得大量代理静态图片文件的请求被丢弃，也就导致很多图片加载不出来。</p>
<p>虽然根因不是 k8s 导致的问题，但也从中挖出一些在高并发场景下值得优化的点，请继续往下看。</p>
<h2 id="somaxconn-的默认值很小">somaxconn 的默认值很小</h2>
<p>我们再看下之前 <code>ss -lnt</code> 的输出:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ss -lnt
State      Recv-Q Send-Q Local Address:Port                Peer Address:Port
LISTEN     <span style="color:#ae81ff">129</span>    <span style="color:#ae81ff">128</span>                *:80                             *:*
</code></pre></div><p>仔细一看，<code>Send-Q</code> 表示 accept queue 最大的大小，才 128 ？也太小了吧！</p>
<p>根据前面的介绍我们知道，accept queue 的最大大小会受 <code>net.core.somaxconn</code> 内核参数的限制，我们看下 pod 所在节点上这个内核参数的大小:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cat /proc/sys/net/core/somaxconn
<span style="color:#ae81ff">32768</span>
</code></pre></div><p>是 32768，挺大的，为什么这里 accept queue 最大大小就只有 128 了呢？</p>
<p><code>net.core.somaxconn</code> 这个内核参数是 namespace 隔离了的，我们在容器 netns 中再确认了下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cat /proc/sys/net/core/somaxconn
<span style="color:#ae81ff">128</span>
</code></pre></div><p>为什么只有 128？看下 stackoverflow <a href="https://stackoverflow.com/questions/26177059/refresh-net-core-somaxcomm-or-any-sysctl-property-for-docker-containers/26197875#26197875">这里</a> 的讨论:</p>
<p><code>The &quot;net/core&quot; subsys is registered per network namespace. And the initial value for somaxconn is set to 128.</code></p>
<p>原来新建的 netns 中 somaxconn 默认就为 128，在 <code>include/linux/socket.h</code> 中可以看到这个常量的定义:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#75715e">/* Maximum queue length specifiable by listen.  */</span>
<span style="color:#75715e">#define SOMAXCONN	128
</span></code></pre></div><p>很多人在使用 k8s 时都没太在意这个参数，为什么大家平常在较高并发下也没发现有问题呢？</p>
<p>因为通常进程 <code>accept()</code> 都是很快的，所以一般 accept queue 基本都没什么积压的数据，也就不会溢出导致丢包了。</p>
<p>对于并发量很高的应用，还是建议将 somaxconn 调高。虽然可以进入容器 netns 后使用 <code>sysctl -w net.core.somaxconn=1024</code> 或 <code>echo 1024 &gt; /proc/sys/net/core/somaxconn</code> 临时调整，但调整的意义不大，因为容器内的进程一般在启动的时候才会调用 <code>listen()</code>，然后 accept queue 的大小就被决定了，并且不再改变。</p>
<p>下面介绍几种调整方式:</p>
<h3 id="方式一-使用-k8s-sysctls-特性直接给-pod-指定内核参数">方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数</h3>
<p>示例 yaml:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#66d9ef">apiVersion</span>: v1
<span style="color:#66d9ef">kind</span>: Pod
<span style="color:#66d9ef">metadata</span>:
  <span style="color:#66d9ef">name</span>: sysctl-example
<span style="color:#66d9ef">spec</span>:
  <span style="color:#66d9ef">securityContext</span>:
    <span style="color:#66d9ef">sysctls</span>:
    - <span style="color:#66d9ef">name</span>: net.core.somaxconn
      <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;8096&#34;</span>
</code></pre></div><p>有些参数是 <code>unsafe</code> 类型的，不同环境不一样，我的环境里是可以直接设置 pod 的 <code>net.core.somaxconn</code> 这个 sysctl 的。如果你的环境不行，请参考官方文档 <a href="https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/#enabling-unsafe-sysctls">Using sysctls in a Kubernetes Cluster</a> 启用 <code>unsafe</code> 类型的 sysctl。</p>
<blockquote>
<p>注：此特性在 k8s v1.12 beta，默认开启。</p>
</blockquote>
<h3 id="方式二-使用-initcontainers-设置内核参数">方式二: 使用 initContainers 设置内核参数</h3>
<p>示例 yaml:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#66d9ef">apiVersion</span>: v1
<span style="color:#66d9ef">kind</span>: Pod
<span style="color:#66d9ef">metadata</span>:
  <span style="color:#66d9ef">name</span>: sysctl-example-init
<span style="color:#66d9ef">spec</span>:
  <span style="color:#66d9ef">initContainers</span>:
  - <span style="color:#66d9ef">image</span>: busybox
    <span style="color:#66d9ef">command</span>:
    - sh
    - -c
    - echo <span style="color:#ae81ff">1024</span> &gt; /proc/sys/net/core/somaxconn
    <span style="color:#66d9ef">imagePullPolicy</span>: Always
    <span style="color:#66d9ef">name</span>: setsysctl
    <span style="color:#66d9ef">securityContext</span>:
      <span style="color:#66d9ef">privileged</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#66d9ef">Containers</span>:
  ...
</code></pre></div><blockquote>
<p>注: init container 需要 privileged 权限。</p>
</blockquote>
<h3 id="方式三-安装-tuning-cni-插件统一设置-sysctl">方式三: 安装 tuning CNI 插件统一设置 sysctl</h3>
<p>tuning plugin 地址: <a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/tuning">https://github.com/containernetworking/plugins/tree/master/plugins/meta/tuning</a></p>
<p>CNI 配置示例:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">{</span>
  <span style="color:#e6db74">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;mytuning&#34;</span>,
  <span style="color:#e6db74">&#34;type&#34;</span>: <span style="color:#e6db74">&#34;tuning&#34;</span>,
  <span style="color:#e6db74">&#34;sysctl&#34;</span>: <span style="color:#f92672">{</span>
          <span style="color:#e6db74">&#34;net.core.somaxconn&#34;</span>: <span style="color:#e6db74">&#34;1024&#34;</span>
  <span style="color:#f92672">}</span>
<span style="color:#f92672">}</span>
</code></pre></div><h2 id="nginx-的-backlog">nginx 的 backlog</h2>
<p>我们使用方式一尝试给 nginx pod 的 somaxconn 调高到 8096 后观察:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ss -lnt
State      Recv-Q Send-Q Local Address:Port                Peer Address:Port
LISTEN     <span style="color:#ae81ff">512</span>    <span style="color:#ae81ff">511</span>                *:80                             *:*
</code></pre></div><p>WTF? 还是溢出了，而且调高了 somaxconn 之后虽然 accept queue 的最大大小 (<code>Send-Q</code>) 变大了，但跟 8096 还差很远呀！</p>
<p>在经过一番研究，发现 nginx 在 <code>listen()</code> 时并没有读取 somaxconn 作为 backlog 默认值传入，它有自己的默认值，也支持在配置里改。通过 <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html">ngx_http_core_module</a> 的官方文档我们可以看到它在 linux 下的默认值就是 511:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">backlog=number
   sets the backlog parameter in the listen() call that limits the maximum length for the queue of pending connections. By default, backlog is set to -1 on FreeBSD, DragonFly BSD, and macOS, and to 511 on other platforms.
</code></pre></div><p>配置示例:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">listen  <span style="color:#ae81ff">80</span>  default  backlog<span style="color:#f92672">=</span>1024;
</code></pre></div><p>所以，在容器中使用 nginx 来支撑高并发的业务时，记得要同时调整下 <code>net.core.somaxconn</code> 内核参数和 <code>nginx.conf</code> 中的 backlog 配置。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>Using sysctls in a Kubernetes Cluster: <a href="https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/">https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/</a></li>
<li>SYN packet handling in the wild: <a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/">https://blog.cloudflare.com/syn-packet-handling-in-the-wild/</a></li>
</ul>
            </article>

            <ul class="pager blog-pager">
                
                <li class="previous">
                    <a href="https://imroc.io/posts/kubernetes-no-route-to-host/" data-toggle="tooltip" data-placement="top" title="Kubernetes 疑难杂症排查分享: 诡异的 No route to host">&larr; 前一篇</a>
                </li>
                 
            </ul>

          
            <div class="entry-shang text-center">
    <p>「真诚赞赏，手留余香」</p>
    <button class="zs show-zs btn btn-bred">赞赏</button>
</div>
<div class="zs-modal-bg"></div>
<div class="zs-modal-box">
    <div class="zs-modal-head">
        <button type="button" class="close">×</button>
        <span class="author"><img src="/favicon.png"/>roc</span>
        <p class="tip"><i></i><span>请我喝杯咖啡？</span></p>
    </div>
    <div class="zs-modal-body">
        <div class="zs-modal-btns">
            <button class="btn btn-blink" data-num="2">2元</button>
            <button class="btn btn-blink" data-num="5">5元</button>
            <button class="btn btn-blink" data-num="10">10元</button>
            <button class="btn btn-blink" data-num="50">50元</button>
            <button class="btn btn-blink" data-num="100">100元</button>
            <button class="btn btn-blink" data-num="1">任意金额</button>
        </div>
        <div class="zs-modal-pay">
            <button class="btn btn-bred" id="pay-text">2元</button>
            <p>使用<span id="pay-type">微信</span>扫描二维码完成支付</p>
            <img src="/img/wechat-2.png" id="pay-image"/>
        </div>
    </div>
    <div class="zs-modal-footer">
        <label>
            <input type="radio" name="zs-type" value="wechat" class="zs-type" checked="checked">
            <span class="zs-wechat"><img src="/img/wechat-btn.png" /></span>
        </label>
        <label>
            <input type="radio" name="zs-type" value="alipay" class="zs-type">
            <span class="zs-alipay"><img src="/img/alipay-btn.png" /></span>
        </label>
    </div>
</div>
          

          
            

  <h3>See Also</h3>
  <ul style="margin-bottom: 25px;">
      
      <li><a href="/posts/kubernetes-no-route-to-host/">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a></li>
      
      <li><a href="/posts/kubernetes-service-topology/">k8s v1.17 新特性预告: 拓扑感知服务路由</a></li>
      
      <li><a href="/posts/troubleshooting-with-kubernetes-network/">Kubernetes 网络疑难杂症排查分享</a></li>
      
      <li><a href="/posts/lost-packets-once-enable-tcp-tw-recycle/">Kubernetes 踩坑分享：开启tcp_tw_recycle内核参数在NAT环境会丢包</a></li>
      
      <li><a href="/posts/handle-memory-fragmentation/">Kubernetes 最佳实践：处理内存碎片化</a></li>
      
      <li><a href="/posts/kubernetes-scale-keepalive-service/">Kubernetes 最佳实践：解决长连接服务扩容失效</a></li>
      
      <li><a href="/posts/capture-packets-in-container/">Kubernetes 问题定位技巧：容器内抓包</a></li>
      
      <li><a href="/posts/efficient-kubectl/">kubectl 高效技巧</a></li>
      
      <li><a href="/posts/kubernetes-wildcard-domain-forward/">Kubernetes 泛域名动态 Service 转发解决方案</a></li>
      
      <li><a href="/posts/kubernetes-analysis-exitcode/">Kubernetes 问题定位技巧：分析 ExitCode</a></li>
      
  </ul>

          

          
            
<div id="gitalk-container"></div>
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script>
  
  
  console.log("id:kubernetes-overflow-and-drop-zh")
  var gitalk = new Gitalk({
      clientID: 'd05b8f959b3111c6fb63',
      clientSecret: 'f30ef76ea0050ed731ace76f7fbf684ae5d28fbc',
      repo: 'imroc.github.io',
      owner: 'imroc',
      admin: 'imroc',
      labels: ['Gitalk'],
      title: 'Kubernetes 疑难杂症排查分享：神秘的溢出与丢包',
      createIssueManually: false,
      id: 'kubernetes-overflow-and-drop-zh',
      distractionFreeMode: true
  });
  gitalk.render('gitalk-container');
</script>

          
        </div>
    </div>
</div>

    <footer>
  <div id="copyright">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
          <ul class="list-inline text-center footer-links">
            
                <li>
                  <a href="mailto:roc@imroc.io" title="Email me">
                    <span class="fa-stack fa-lg">
                      <i class="fas fa-circle fa-stack-2x"></i>
                      <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                    </span>
                  </a>
                </li>
                <li>
                  <a href="https://github.com/imroc" title="GitHub">
                    <span class="fa-stack fa-lg">
                      <i class="fas fa-circle fa-stack-2x"></i>
                      <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                    </span>
                  </a>
                </li>
                <li>
                  <a href="https://twitter.com/imrocchan" title="Twitter">
                    <span class="fa-stack fa-lg">
                      <i class="fas fa-circle fa-stack-2x"></i>
                      <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                    </span>
                  </a>
                </li>
            
            <li>
              <a href="https://imroc.io/index.xml" title="RSS">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            
          </ul>
          <p class="credits copyright text-muted">
          &copy;2017-2020
            
              
                <a href="/about">roc</a>
              
            
            &nbsp;&bull;&nbsp;
            2020-01-12
            更新
          </p>
          <p class="credits theme-by text-muted">
            由 <a href="http://gohugo.io">Hugo v0.65.3</a> 强力驱动 &nbsp;&bull;&nbsp; 主题 <a href="https://github.com/imroc/xhugo">xhugo</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  window.copyText='复制'==''?"Copy":'复制'
  window.copiedText='已复制!'==''?"Copied":'已复制!'
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js"
  integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js"
  integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://cdn.bootcss.com/jquery/1.12.4/jquery.min.js"
  integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://cdn.bootcss.com/twitter-bootstrap/3.3.7/js/bootstrap.min.js"
  integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>





<script src='/js/bundle.min.3e2cb25916e0577d6eaf45f0f6125a6156723011439209b222abd8207bb6057f.js' integrity='sha256-PiyyWRbgV31ur0Xw9hJaYVZyMBFDkgmyIqvYIHu2BX8='></script>






  </body>
</html>

