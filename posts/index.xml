<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on roc</title>
    <link>https://imroc.io/posts/</link>
    <description>Recent content in Posts on roc</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <managingEditor>roc@imroc.io (roc)</managingEditor>
    <webMaster>roc@imroc.io (roc)</webMaster>
    <lastBuildDate>Fri, 07 Aug 2020 11:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://imroc.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Nginx Ingress on TKE 部署最佳实践</title>
      <link>https://imroc.io/posts/nginx-on-tke/</link>
      <pubDate>Fri, 07 Aug 2020 11:00:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/nginx-on-tke/</guid>
      <description>概述 开源的 Ingress Controller 的实现使用量最大的莫过于 Nginx Ingress 了，功能强大且性能极高。Nginx Ingress 有多种部署方式，本文将介绍 Nginx Ingress 在 TKE 上的一些部署方案，这几种方案</description>
    </item>
    
    <item>
      <title>Kubernetes 服务部署最佳实践(二) 如何提高服务可用性</title>
      <link>https://imroc.io/posts/kubernetes-app-deployment-best-practice-2/</link>
      <pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-app-deployment-best-practice-2/</guid>
      <description>引言 上一篇 文章我们围绕如何合理利用资源的主题做了一些最佳实践的分享，这一次我们就如何提高服务可用性的主题来展开探讨。 怎样提高我们部署服务的可</description>
    </item>
    
    <item>
      <title>Kubernetes 服务部署最佳实践(一) 如何合理利用资源</title>
      <link>https://imroc.io/posts/kubernetes-app-deployment-best-practice-1/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-app-deployment-best-practice-1/</guid>
      <description>引言 业务容器化后，如何将其部署在 K8S 上？如果仅仅是将它跑起来，很简单，但如果是上生产，我们有许多地方是需要结合业务场景和部署环境进行方案选型和</description>
    </item>
    
    <item>
      <title>TKE 集群组建最佳实践</title>
      <link>https://imroc.io/posts/tke-cluster-setup-best-practice/</link>
      <pubDate>Mon, 25 May 2020 10:03:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/tke-cluster-setup-best-practice/</guid>
      <description>Kubernetes 版本 K8S 版本迭代比较快，新版本通常包含许多 bug 修复和新功能，旧版本逐渐淘汰，建议创建集群时选择当前 TKE 支持的最新版本，后续出新版本后也是可以支持</description>
    </item>
    
    <item>
      <title>打造云原生大型分布式监控系统(三): Thanos 部署与实践</title>
      <link>https://imroc.io/posts/build-cloud-native-large-scale-distributed-monitoring-system-3/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/build-cloud-native-large-scale-distributed-monitoring-system-3/</guid>
      <description>&lt;h2 id=&#34;视频&#34;&gt;视频&lt;/h2&gt;
&lt;p&gt;附上本系列完整视频&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;打造云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段 &lt;a href=&#34;https://www.bilibili.com/video/BV17C4y1x7HE&#34;&gt;https://www.bilibili.com/video/BV17C4y1x7HE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;打造云原生大型分布式监控系统(二): Thanos 架构详解 &lt;a href=&#34;https://www.bilibili.com/video/BV1Vk4y1R7S9&#34;&gt;https://www.bilibili.com/video/BV1Vk4y1R7S9&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;打造云原生大型分布式监控系统(三): Thanos 部署与实践 &lt;a href=&#34;https://www.bilibili.com/video/BV16g4y187HD&#34;&gt;https://www.bilibili.com/video/BV16g4y187HD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;上一篇 &lt;a href=&#34;../build-cloud-native-large-scale-distributed-monitoring-system-2&#34;&gt;Thanos 架构详解&lt;/a&gt; 我们深入理解了 thanos 的架构设计与实现原理，现在我们来聊聊实战，分享一下如何部署和使用 Thanos。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>打造云原生大型分布式监控系统(二): Thanos 架构详解</title>
      <link>https://imroc.io/posts/build-cloud-native-large-scale-distributed-monitoring-system-2/</link>
      <pubDate>Mon, 06 Apr 2020 13:50:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/build-cloud-native-large-scale-distributed-monitoring-system-2/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;之前在 &lt;a href=&#34;../build-cloud-native-large-scale-distributed-monitoring-system-1&#34;&gt;大规模场景下 Prometheus 的优化手段&lt;/a&gt; 中，我们想尽 &amp;ldquo;千方百计&amp;rdquo; 才好不容易把 Prometheus 优化到适配大规模场景，部署和后期维护麻烦且复杂不说，还有很多不完美的地方，并且还无法满足一些更高级的诉求，比如查看时间久远的监控数据，对于一些时间久远不常用的 &amp;ldquo;冷数据&amp;rdquo;，最理想的方式就是存到廉价的对象存储中，等需要查询的时候能够自动加载出来。&lt;/p&gt;
&lt;p&gt;Thanos (没错，就是灭霸) 可以帮我们简化分布式 Prometheus 的部署与管理，并提供了一些的高级特性：&lt;strong&gt;全局视图&lt;/strong&gt;，&lt;strong&gt;长期存储&lt;/strong&gt;，&lt;strong&gt;高可用&lt;/strong&gt;。下面我们来详细讲解一下。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>打造云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段</title>
      <link>https://imroc.io/posts/build-cloud-native-large-scale-distributed-monitoring-system-1/</link>
      <pubDate>Thu, 26 Mar 2020 22:50:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/build-cloud-native-large-scale-distributed-monitoring-system-1/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;Prometheus 几乎已成为监控领域的事实标准，它自带高效的时序数据库存储，可以让单台 Prometheus 能够高效的处理大量的数据，还有友好并且强大的 PromQL 语法，可以用来灵活的查询各种监控数据以及配置告警规则，同时它的 pull 模型指标采集方式被广泛采纳，非常多的应用都实现了 Prometheus 的 metrics 接口以暴露自身各项数据指标让 Prometheus 去采集，很多没有适配的应用也会有第三方 exporter 帮它去适配 Prometheus，所以监控系统我们通常首选用 Prometheus，本系列文章也将基于 Prometheus 来打造云原生环境下的大型分布式监控系统。&lt;/p&gt;
&lt;h2 id=&#34;大规模场景下-prometheus-的痛点&#34;&gt;大规模场景下 Prometheus 的痛点&lt;/h2&gt;
&lt;p&gt;Prometheus 本身只支持单机部署，没有自带支持集群部署，也就不支持高可用以及水平扩容，在大规模场景下，最让人关心的问题是它的存储空间也受限于单机磁盘容量，磁盘容量决定了单个 Prometheus 所能存储的数据量，数据量大小又取决于被采集服务的指标数量、服务数量、采集速率以及数据过期时间。在数据量大的情况下，我们可能就需要做很多取舍，比如丢弃不重要的指标、降低采集速率、设置较短的数据过期时间(默认只保留15天的数据，看不到比较久远的监控数据)。&lt;/p&gt;
&lt;p&gt;这些痛点实际也是可以通过一些优化手段来改善的，下面我们来细讲一下。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</title>
      <link>https://imroc.io/posts/kubernetes-overflow-and-drop/</link>
      <pubDate>Sun, 12 Jan 2020 19:20:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-overflow-and-drop/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;上一篇 &lt;a href=&#34;https://imroc.io/posts/kubernetes/kubernetes-no-route-to-host/&#34;&gt;Kubernetes 疑难杂症排查分享: 诡异的 No route to host&lt;/a&gt; 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;问题描述&#34;&gt;问题描述&lt;/h2&gt;
&lt;p&gt;有用户反馈大量图片加载不出来。&lt;/p&gt;
&lt;p&gt;图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client &amp;ndash;&amp;gt; k8s ingress &amp;ndash;&amp;gt; nginx &amp;ndash;&amp;gt; nfs。&lt;/p&gt;
&lt;h2 id=&#34;猜测&#34;&gt;猜测&lt;/h2&gt;
&lt;p&gt;猜测: ingress 图片下载路径对应的后端服务出问题了。&lt;/p&gt;
&lt;p&gt;验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！&lt;/p&gt;
&lt;h2 id=&#34;抓包&#34;&gt;抓包&lt;/h2&gt;
&lt;p&gt;继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 拿到 pod 中 nginx 的容器 id&lt;/span&gt;
$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;^Containers:&amp;#34;&lt;/span&gt; | grep -Eo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;docker://.*$&amp;#39;&lt;/span&gt; | head -n &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; | sed &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;s/docker:\/\/\(.*\)$/\1/&amp;#39;&lt;/span&gt;
49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e

&lt;span style=&#34;color:#75715e&#34;&gt;# 通过容器 id 拿到 nginx 进程 pid&lt;/span&gt;
$ docker inspect -f &lt;span style=&#34;color:#f92672&#34;&gt;{{&lt;/span&gt;.State.Pid&lt;span style=&#34;color:#f92672&#34;&gt;}}&lt;/span&gt; 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e
&lt;span style=&#34;color:#ae81ff&#34;&gt;3985&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# 进入 nginx 进程所在的 netns&lt;/span&gt;
$ nsenter -n -t &lt;span style=&#34;color:#ae81ff&#34;&gt;3985&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# 查看容器 netns 中的网卡信息，确认下&lt;/span&gt;
$ ip a
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;65536&lt;/span&gt; qdisc noqueue state UNKNOWN group default qlen &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if11: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;1500&lt;/span&gt; qdisc noqueue state UP group default
    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    inet 172.26.0.8/26 scope global eth0
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;tcpdump -i eth0 -nnnn -ttt port &lt;span style=&#34;color:#ae81ff&#34;&gt;24568&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在其它节点准备使用 nc 指定源端口为 24568 向容器发包：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;nc -u &lt;span style=&#34;color:#ae81ff&#34;&gt;24568&lt;/span&gt; 172.16.1.21 &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;观察抓包结果：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;00:00:00.000000 IP 10.0.0.3.24568 &amp;gt; 172.16.1.21.80: Flags &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, seq 416500297, win 29200, options &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;mss 1424,sackOK,TS val &lt;span style=&#34;color:#ae81ff&#34;&gt;3000206334&lt;/span&gt; ecr 0,nop,wscale 9&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
00:00:01.032218 IP 10.0.0.3.24568 &amp;gt; 172.16.1.21.80: Flags &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, seq 416500297, win 29200, options &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;mss 1424,sackOK,TS val &lt;span style=&#34;color:#ae81ff&#34;&gt;3000207366&lt;/span&gt; ecr 0,nop,wscale 9&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
00:00:02.011962 IP 10.0.0.3.24568 &amp;gt; 172.16.1.21.80: Flags &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, seq 416500297, win 29200, options &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;mss 1424,sackOK,TS val &lt;span style=&#34;color:#ae81ff&#34;&gt;3000209378&lt;/span&gt; ecr 0,nop,wscale 9&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
00:00:04.127943 IP 10.0.0.3.24568 &amp;gt; 172.16.1.21.80: Flags &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, seq 416500297, win 29200, options &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;mss 1424,sackOK,TS val &lt;span style=&#34;color:#ae81ff&#34;&gt;3000213506&lt;/span&gt; ecr 0,nop,wscale 9&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
00:00:08.192056 IP 10.0.0.3.24568 &amp;gt; 172.16.1.21.80: Flags &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, seq 416500297, win 29200, options &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;mss 1424,sackOK,TS val &lt;span style=&#34;color:#ae81ff&#34;&gt;3000221698&lt;/span&gt; ecr 0,nop,wscale 9&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
00:00:16.127983 IP 10.0.0.3.24568 &amp;gt; 172.16.1.21.80: Flags &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, seq 416500297, win 29200, options &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;mss 1424,sackOK,TS val &lt;span style=&#34;color:#ae81ff&#34;&gt;3000237826&lt;/span&gt; ecr 0,nop,wscale 9&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
00:00:33.791988 IP 10.0.0.3.24568 &amp;gt; 172.16.1.21.80: Flags &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, seq 416500297, win 29200, options &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;mss 1424,sackOK,TS val &lt;span style=&#34;color:#ae81ff&#34;&gt;3000271618&lt;/span&gt; ecr 0,nop,wscale 9&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;, length &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。&lt;/p&gt;
&lt;p&gt;排除是 iptables 规则问题，在容器 netns 中使用 &lt;code&gt;netstat -s&lt;/code&gt; 检查下是否有丢包统计:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ netstat -s | grep -E &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;overflow|drop&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#ae81ff&#34;&gt;12178939&lt;/span&gt; times the listen queue of a socket overflowed
    &lt;span style=&#34;color:#ae81ff&#34;&gt;12247395&lt;/span&gt; SYNs to LISTEN sockets dropped
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 疑难杂症排查分享: 诡异的 No route to host</title>
      <link>https://imroc.io/posts/kubernetes-no-route-to-host/</link>
      <pubDate>Sun, 15 Dec 2019 12:03:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-no-route-to-host/</guid>
      <description>&lt;p&gt;之前发过一篇干货满满的爆火文章 &lt;a href=&#34;https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/&#34;&gt;Kubernetes 网络疑难杂症排查分享&lt;/a&gt;，包含多个疑难杂症的排查案例分享，信息量巨大。这次我又带来了续集，只讲一个案例，但信息量也不小，Are you ready ?&lt;/p&gt;
&lt;h2 id=&#34;问题反馈&#34;&gt;问题反馈&lt;/h2&gt;
&lt;p&gt;有用户反馈 Deployment 滚动更新的时候，业务日志偶尔会报 &amp;ldquo;No route to host&amp;rdquo; 的错误。&lt;/p&gt;
&lt;h2 id=&#34;分析&#34;&gt;分析&lt;/h2&gt;
&lt;p&gt;之前没遇到滚动更新会报 &amp;ldquo;No route to host&amp;rdquo; 的问题，我们先看下滚动更新导致连接异常有哪些常见的报错:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connection reset by peer&lt;/code&gt;: 连接被重置。通常是连接建立过，但 server 端发现 client 发的包不对劲就返回 RST，应用层就报错连接被重置。比如在 server 滚动更新过程中，client 给 server 发的请求还没完全结束，或者本身是一个类似 grpc 的多路复用长连接，当 server 对应的旧 Pod 删除(没有做优雅结束，停止时没有关闭连接)，新 Pod 很快创建启动并且刚好有跟之前旧 Pod 一样的 IP，这时 kube-proxy 也没感知到这个 IP 其实已经被删除然后又被重建了，针对这个 IP 的规则就不会更新，旧的连接依然发往这个 IP，但旧 Pod 已经不在了，后面继续发包时依然转发给这个 Pod IP，最终会被转发到这个有相同 IP 的新 Pod 上，而新 Pod 收到此包时检查报文发现不对劲，就返回 RST 给 client 告知将连接重置。针对这种情况，建议应用自身处理好优雅结束：Pod 进入 Terminating 状态后会发送 &lt;code&gt;SIGTERM&lt;/code&gt; 信号给业务进程，业务进程的代码需处理这个信号，在进程退出前关闭所有连接。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connection refused&lt;/code&gt;: 连接被拒绝。通常是连接还没建立，client 正在发 SYN 包请求建立连接，但到了 server 之后发现端口没监听，内核就返回 RST 包，然后应用层就报错连接被拒绝。比如在 server 滚动更新过程中，旧的 Pod 中的进程很快就停止了(网卡还未完全销毁)，但 client 所在节点的 iptables/ipvs 规则还没更新，包就可能会被转发到了这个停止的 Pod (由于 k8s 的 controller 模式，从 Pod 删除到 service 的 endpoint 更新，再到 kube-proxy watch 到更新并更新 节点上的 iptables/ipvs 规则，这个过程是异步的，中间存在一点时间差，所以有可能存在 Pod 中的进程已经没有监听，但 iptables/ipvs 规则还没更新的情况)。针对这种情况，建议给容器加一个 preStop，在真正销毁 Pod 之前等待一段时间，留时间给 kube-proxy 更新转发规则，更新完之后就不会再有新连接往这个旧 Pod 转发了，preStop 示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;lifecycle&lt;/span&gt;:
  &lt;span style=&#34;color:#66d9ef&#34;&gt;preStop&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;exec&lt;/span&gt;:
      &lt;span style=&#34;color:#66d9ef&#34;&gt;command&lt;/span&gt;:
      - /bin/bash
      - -c
      - sleep &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;另外，还可能是新的 Pod 启动比较慢，虽然状态已经 Ready，但实际上可能端口还没监听，新的请求被转发到这个还没完全启动的 Pod 就会报错连接被拒绝。针对这种情况，建议给容器加就绪检查 (readinessProbe)，让容器真正启动完之后才将其状态置为 Ready，然后 kube-proxy 才会更新转发规则，这样就能保证新的请求只被转发到完全启动的 Pod，readinessProbe 示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;readinessProbe&lt;/span&gt;:
  &lt;span style=&#34;color:#66d9ef&#34;&gt;httpGet&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;path&lt;/span&gt;: /healthz
    &lt;span style=&#34;color:#66d9ef&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;httpHeaders&lt;/span&gt;:
    - &lt;span style=&#34;color:#66d9ef&#34;&gt;name&lt;/span&gt;: X-Custom-Header
      &lt;span style=&#34;color:#66d9ef&#34;&gt;value&lt;/span&gt;: Awesome
  &lt;span style=&#34;color:#66d9ef&#34;&gt;initialDelaySeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;timeoutSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connection timed out&lt;/code&gt;: 连接超时。通常是连接还没建立，client 发 SYN 请求建立连接一直等到超时时间都没有收到 ACK，然后就报错连接超时。这个可能场景跟前面 &lt;code&gt;Connection refused&lt;/code&gt; 可能的场景类似，不同点在于端口有监听，但进程无法正常响应了: 转发规则还没更新，旧 Pod 的进程正在停止过程中，虽然端口有监听，但已经不响应了；或者转发规则更新了，新 Pod 端口也监听了，但还没有真正就绪，还没有能力处理新请求。针对这些情况的建议跟前面一样：加 preStop 和 readinessProbe。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面我们来继续分析下滚动更新时发生 &lt;code&gt;No route to host&lt;/code&gt; 的可能情况。&lt;/p&gt;
&lt;p&gt;这个报错很明显，IP 无法路由，通常是将报文发到了一个已经彻底销毁的 Pod (网卡已经不在)。不可能发到一个网卡还没创建好的 Pod，因为即便不加存活检查，也是要等到 Pod 网络初始化完后才可能 Ready，然后 kube-proxy 才会更新转发规则。&lt;/p&gt;
&lt;p&gt;什么情况下会转发到一个已经彻底销毁的 Pod？ 借鉴前面几种滚动更新的报错分析，我们推测应该是 Pod 很快销毁了但转发规则还没更新，从而新的请求被转发了这个已经销毁的 Pod，最终报文到达这个 Pod 所在 PodCIDR 的 Node 上时，Node 发现本机已经没有这个 IP 的容器，然后 Node 就返回 ICMP 包告知 client 这个 IP 不可达，client 收到 ICMP 后，应用层就会报错 &amp;ldquo;No route to host&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;所以根据我们的分析，关键点在于 Pod 销毁太快，转发规则还没来得及更新，导致后来的请求被转发到已销毁的 Pod。针对这种情况，我们可以给容器加一个 preStop，留时间给 kube-proxy 更新转发规则来解决，参考 《Kubernetes实践指南》中的部分章节: &lt;a href=&#34;https://k8s.imroc.io/best-practice/high-availability-deployment-of-applications#smooth-update-using-prestophook-and-readinessprobe&#34;&gt;https://k8s.imroc.io/best-practice/high-availability-deployment-of-applications#smooth-update-using-prestophook-and-readinessprobe&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s v1.17 新特性预告: 拓扑感知服务路由</title>
      <link>https://imroc.io/posts/kubernetes-service-topology/</link>
      <pubDate>Tue, 26 Nov 2019 16:49:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-service-topology/</guid>
      <description>&lt;p&gt;今天给大家介绍下我参与开发的一个 k8s v1.17 新特性: 拓扑感知服务路由。&lt;/p&gt;
&lt;h2 id=&#34;名词解释&#34;&gt;名词解释&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;拓扑域: 表示在集群中的某一类 &amp;ldquo;地方&amp;rdquo;，比如某节点、某机架、某可用区或某地域等，这些都可以作为某种拓扑域。&lt;/li&gt;
&lt;li&gt;endpoint: k8s 某个服务的某个 ip+port，通常是 pod 的 ip+port。&lt;/li&gt;
&lt;li&gt;service: k8s 的 service 资源(服务)，关联一组 endpoint ，访问 service 会被转发到关联的某个 endpoint 上。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;拓扑感知服务路由，此特性最初由杜军大佬提出并设计。为什么要设计此特性呢？想象一下，k8s 集群节点分布在不同的地方，service 对应的 endpoints 分布在不同节点，传统转发策略会对所有 endpoint 做负载均衡，通常会等概率转发，当访问 service 时，流量就可能被分散打到这些不同的地方。虽然 service 转发做了负载均衡，但如果 endpoint 距离比较远，流量转发过去网络时延就相对比较高，会影响网络性能，在某些情况下甚至还可能会付出额外的流量费用。要是如能实现 service 就近转发 endpoint，是不是就可以实现降低网络时延，提升网络性能了呢？是的！这也正是该特性所提出的目的和意义。&lt;/p&gt;
&lt;h2 id=&#34;k8s-亲和性&#34;&gt;k8s 亲和性&lt;/h2&gt;
&lt;p&gt;service 的就近转发实际就是一种网络的亲和性，倾向于转发到离自己比较近的 endpoint。在此特性之前，已经在调度和存储方面有一些亲和性的设计与实现:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点亲和性 (Node Affinity): 让 Pod 被调度到符合一些期望条件的 Node 上，比如限制调度到某一可用区，或者要求节点支持 GPU，这算是调度亲和，调度结果取决于节点属性。&lt;/li&gt;
&lt;li&gt;Pod 亲和性与反亲和性 (Pod Affinity/AntiAffinity): 让一组 Pod 调度到同一拓扑域的节点上，或者打散到不同拓扑域的节点， 这也算是调度亲和，调度结果取决于其它 Pod。&lt;/li&gt;
&lt;li&gt;数据卷拓扑感知调度 (Volume Topology-aware Scheduling): 让 Pod 只被调度到符合其绑定的存储所在拓扑域的节点上，这算是调度与存储的亲和，调度结果取决于存储的拓扑域。&lt;/li&gt;
&lt;li&gt;本地数据卷 (Local Persistent Volume): 让 Pod 使用本地数据卷，比如高性能 SSD，在某些需要高 IOPS 低时延的场景很有用，它还会保证 Pod 始终被调度到同一节点，数据就不会不丢失，这也算是调度与存储的亲和，调度结果取决于存储所在节点。&lt;/li&gt;
&lt;li&gt;数据卷拓扑感知动态创建 (Topology-Aware Volume Dynamic Provisioning): 先调度 Pod，再根据 Pod 所在节点的拓扑域来创建存储，这算是存储与调度的亲和，存储的创建取决于调度的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而 k8s 目前在网络方面还没有亲和性能力，拓扑感知服务路由这个新特性恰好可以补齐这个的空缺，此特性使得 service 可以实现就近转发而不是所有 endpoint 等概率转发。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 网络疑难杂症排查分享</title>
      <link>https://imroc.io/posts/troubleshooting-with-kubernetes-network/</link>
      <pubDate>Mon, 12 Aug 2019 16:59:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/troubleshooting-with-kubernetes-network/</guid>
      <description>&lt;p&gt;大家好，我是 roc，来自腾讯云容器服务(TKE)团队，经常帮助用户解决各种 K8S 的疑难杂症，积累了比较丰富的经验，本文分享几个比较复杂的网络方面的问题排查和解决思路，深入分析并展开相关知识，信息量巨大，相关经验不足的同学可能需要细细品味才能消化，我建议收藏本文反复研读，当完全看懂后我相信你的功底会更加扎实，解决问题的能力会大大提升。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文发现的问题是在使用 TKE 时遇到的，不同厂商的网络环境可能不一样，文中会对不同的问题的网络环境进行说明&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 踩坑分享：开启tcp_tw_recycle内核参数在NAT环境会丢包</title>
      <link>https://imroc.io/posts/lost-packets-once-enable-tcp-tw-recycle/</link>
      <pubDate>Sun, 09 Jun 2019 22:00:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/lost-packets-once-enable-tcp-tw-recycle/</guid>
      <description>原因 tcp_tw_recycle参数。它用来快速回收TIME_WAIT连接，不过如果在NAT环境下会引发问题。 RFC1323中有如下一段描述</description>
    </item>
    
    <item>
      <title>Kubernetes 最佳实践：处理内存碎片化</title>
      <link>https://imroc.io/posts/handle-memory-fragmentation/</link>
      <pubDate>Sat, 08 Jun 2019 13:59:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/handle-memory-fragmentation/</guid>
      <description>内存碎片化造成的危害 节点的内存碎片化严重，导致docker运行容器时，无法分到大的内存块，导致start docker失败。最终导致服务更新时</description>
    </item>
    
    <item>
      <title>Kubernetes 最佳实践：解决长连接服务扩容失效</title>
      <link>https://imroc.io/posts/kubernetes-scale-keepalive-service/</link>
      <pubDate>Thu, 06 Jun 2019 17:06:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-scale-keepalive-service/</guid>
      <description>在现网运营中，有很多场景为了提高效率，一般都采用建立长连接的方式来请求。我们发现在客户端以长连接请求服务端的场景下，K8S的自动扩容会失效。</description>
    </item>
    
    <item>
      <title>Kubernetes 问题定位技巧：容器内抓包</title>
      <link>https://imroc.io/posts/capture-packets-in-container/</link>
      <pubDate>Sun, 19 May 2019 11:24:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/capture-packets-in-container/</guid>
      <description>在使用 kubernetes 跑应用的时候，可能会遇到一些网络问题，比较常见的是服务端无响应(超时)或回包内容不正常，如果没找出各种配置上有问题，这时我们需要确认</description>
    </item>
    
    <item>
      <title>Istio 学习笔记：Istio CNI 插件</title>
      <link>https://imroc.io/posts/istio-cni/</link>
      <pubDate>Sun, 07 Apr 2019 11:54:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/istio-cni/</guid>
      <description>设计目标 当前实现将用户 pod 流量转发到 proxy 的默认方式是使用 privileged 权限的 istio-init 这个 init container 来做的（运行脚本写入 iptables），Istio CNI 插件的主要设计目标</description>
    </item>
    
    <item>
      <title>kubectl 高效技巧</title>
      <link>https://imroc.io/posts/efficient-kubectl/</link>
      <pubDate>Sun, 10 Mar 2019 14:05:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/efficient-kubectl/</guid>
      <description>是否有过因为使用 kubectl 经常需要重复输入命名空间而苦恼？是否觉得应该要有个记住命名空间的功能，自动记住上次使用的命名空间，不需要每次都输入？可惜没</description>
    </item>
    
    <item>
      <title>Kubernetes 泛域名动态 Service 转发解决方案</title>
      <link>https://imroc.io/posts/kubernetes-wildcard-domain-forward/</link>
      <pubDate>Sat, 22 Dec 2018 01:09:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-wildcard-domain-forward/</guid>
      <description>需求 集群对外暴露了一个公网IP作为流量入口(可以是 Ingress 或 Service)，DNS 解析配置了一个泛域名指向该IP（比如 *.test.imroc.</description>
    </item>
    
    <item>
      <title>Kubernetes 问题定位技巧：分析 ExitCode</title>
      <link>https://imroc.io/posts/kubernetes-analysis-exitcode/</link>
      <pubDate>Fri, 21 Dec 2018 16:10:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-analysis-exitcode/</guid>
      <description>使用 kubectl describe pod 查看异常的 pod 的状态，在容器列表里看 State 字段，其中 ExitCode 即程序退出时的状态码，正常退出时为0。如果不为0，表示异常退出，我们可以分析下原因</description>
    </item>
    
    <item>
      <title>Git技巧：修改历史</title>
      <link>https://imroc.io/posts/git-trick-modify-history/</link>
      <pubDate>Fri, 07 Dec 2018 18:05:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/git-trick-modify-history/</guid>
      <description>修改历史 修改最新一条历史 如果内容需要改就直接改，然后 git add 进去，然后执行 git commit --amend 会弹出 git commit message 的编辑窗口，会填充之前 commit 时写的 message 内容，如果需要改就直</description>
    </item>
    
    <item>
      <title>教你如何全键盘操作 Chrome 浏览器</title>
      <link>https://imroc.io/posts/chrome/</link>
      <pubDate>Sun, 21 Oct 2018 23:00:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/chrome/</guid>
      <description>推荐两款插件， SurfingKeys 和 Steward，让你全键盘高效操作浏览器。老规矩，附视频教学。 SurfingKeys 模拟 vim 的快捷键有两款 Chrome 插件，分别是 Vimium 和 Surfingkey</description>
    </item>
    
    <item>
      <title>极客工具之 Alfred 与 Dash</title>
      <link>https://imroc.io/posts/alfred-and-dash/</link>
      <pubDate>Sun, 21 Oct 2018 07:00:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/alfred-and-dash/</guid>
      <description>Alfred 使用 Alfred 可以让你在 macOS 程序间自由切换、快速查找或打开文件、调起浏览器进行网页搜索、 还可以做计算器。 另外，还有许多其它搜索功能以及付费的工作流特</description>
    </item>
    
    <item>
      <title>极客工具之 oh-my-zsh</title>
      <link>https://imroc.io/posts/oh-my-zsh/</link>
      <pubDate>Sat, 20 Oct 2018 01:35:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/oh-my-zsh/</guid>
      <description>shell 有多种，大多数人接触比较多的是 bash， 不管是 mac 还是各个 linux 发行版，默认的 shell 基本都是 bash，虽然 bash 功能已经丰富了，但对于极客们来说，界面不</description>
    </item>
    
    <item>
      <title>通俗理解Kubernetes中Service、Ingress与Ingress Controller的作用与关系</title>
      <link>https://imroc.io/posts/understand-service-ingress-and-ingress-controller/</link>
      <pubDate>Tue, 24 Jul 2018 22:19:37 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/understand-service-ingress-and-ingress-controller/</guid>
      <description>通俗的讲: Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务 Ingress 是反向代理规则，用来规定 HTTP/S 请求应该被转发到哪个 Service 上，比如根据请求中不同的</description>
    </item>
    
    <item>
      <title>利用Helm一键部署Kubernetes Dashboard并启用免费HTTPS</title>
      <link>https://imroc.io/posts/deploy-kubernetes-dashboard-and-enable-free-https/</link>
      <pubDate>Mon, 23 Jul 2018 21:49:54 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/deploy-kubernetes-dashboard-and-enable-free-https/</guid>
      <description>概述 Kubernetes Dashboard 是一个可以可视化查看和操作 Kubernetes 集群的一个插件 本文利用 Helm 部署它，所以请确保 Helm 已安装，安装方法参考：https://imroc.io/po</description>
    </item>
    
    <item>
      <title>利用cert-manager让Ingress启用免费的HTTPS证书</title>
      <link>https://imroc.io/posts/let-ingress-enable-free-https-with-cert-manager/</link>
      <pubDate>Mon, 23 Jul 2018 20:08:01 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/let-ingress-enable-free-https-with-cert-manager/</guid>
      <description>概述 cert-manager 是替代 kube-lego 的一个开源项目，用于在 Kubernetes 集群中自动提供 HTTPS 证书，支持 Let’s Encrypt, HashiCorp Vault 这些免费证书的签发。 本文使用 Helm 安装，所以请确保 Helm 已安装，安装</description>
    </item>
    
    <item>
      <title>使用Nginx Ingress Controller导入外部流量到Kubernetes集群内部</title>
      <link>https://imroc.io/posts/use-nginx-ingress-controller-to-expose-service/</link>
      <pubDate>Mon, 23 Jul 2018 14:29:37 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/use-nginx-ingress-controller-to-expose-service/</guid>
      <description>概述 Nginx Ingress Controller 是 Kubernetes Ingress Controller 的一种实现，作为反向代理将外部流量导入集群内部，实现将 Kubernetes 内部的 Service 暴露给外部，这样我们就能通过公网或内网直接访问集群内部的服</description>
    </item>
    
    <item>
      <title>利用Gitlab和Jenkins做CI(持续集成)</title>
      <link>https://imroc.io/posts/ci-with-gitlab-and-jenkins/</link>
      <pubDate>Fri, 13 Apr 2018 22:23:22 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/ci-with-gitlab-and-jenkins/</guid>
      <description>利用Gitlab和Jenkins做CI(持续集成) 最近用到持续集成顺便总结在这里，都是用的最新版。搭建过程中还有一个demo，提交代码到 gitlab 自</description>
    </item>
    
    <item>
      <title>对比Kubernetes的Nodeport、Loadbalancer和Ingress，什么时候该用哪种</title>
      <link>https://imroc.io/posts/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what/</link>
      <pubDate>Tue, 13 Mar 2018 22:45:26 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what/</guid>
      <description>本文翻译自：https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalan</description>
    </item>
    
    <item>
      <title>kubernetes源码阅读笔记：理清 kube-apiserver 的源码主线</title>
      <link>https://imroc.io/posts/kubernetes-source-code-reading-notes-kube-apiserver-code-main-line/</link>
      <pubDate>Mon, 12 Mar 2018 11:47:19 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-source-code-reading-notes-kube-apiserver-code-main-line/</guid>
      <description>前言 我最近开始研究 kubernetes 源码，希望将阅读笔记记录下来，分享阅读思路和心得，更好的理解 kubernetes，这是第一篇，从 kube-apiserver 开始。 开始 k8s各组件</description>
    </item>
    
    <item>
      <title>利用Katacoda免费同步Docker镜像到Docker Hub</title>
      <link>https://imroc.io/posts/sync-images-to-docker-hub-using-katacoda/</link>
      <pubDate>Fri, 09 Mar 2018 10:39:17 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/sync-images-to-docker-hub-using-katacoda/</guid>
      <description>为什么要同步 安装kubernetes的时候，我们需要用到 gcr.io/google_containers 下面的一些镜像，在国内是不能直接下载的。如果用 Self Host 方式安装，master 上的组件除</description>
    </item>
    
    <item>
      <title>利用Hugo和Github Pages免费创建并永久托管网站</title>
      <link>https://imroc.io/posts/building-website-for-free-using-hugo-and-github-pages/</link>
      <pubDate>Tue, 16 Jan 2018 20:35:06 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/building-website-for-free-using-hugo-and-github-pages/</guid>
      <description>概述 Hugo可以让你轻松生成静态网站，比如个人博客、API文档、公司主页等，你只需要提供markdown格式的文本，它就能帮你渲染成各种你想</description>
    </item>
    
    <item>
      <title>golang中append函数返回值必须有变量接收的原因探究</title>
      <link>https://imroc.io/posts/golang-append/</link>
      <pubDate>Wed, 06 Sep 2017 15:35:18 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/golang-append/</guid>
      <description>append函数返回更新后的slice（长度和容量可能会变），必须重新用slice的变量接收，不然无法编译通过 为了弄明白为什么，首先我们需要</description>
    </item>
    
    <item>
      <title>Go语言(golang)包设计哲学-原则与项目结构组织最佳实践</title>
      <link>https://imroc.io/posts/golang-package/</link>
      <pubDate>Wed, 06 Sep 2017 15:35:18 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/golang-package/</guid>
      <description>总结下Go的package设计哲学 明确目的 在准备设计一个包之前，我们需要明确它的目的。 包的命名就必须明确体现其目的，而不仅仅是为了存放代码。</description>
    </item>
    
    <item>
      <title>Go语言技巧-使用for range time.Tick()固定间隔时间执行</title>
      <link>https://imroc.io/posts/golang-for-range-time-tick/</link>
      <pubDate>Wed, 06 Sep 2017 15:35:18 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/golang-for-range-time-tick/</guid>
      <description>直接上代码: for range time.Tick(30 * time.Millisecond) { doSomthing() } 因为time.Tick()返回的是一个channel,每隔指定的时间会有数据从channel中出来，for ran</description>
    </item>
    
    <item>
      <title>Go语言技巧-使用select{}阻塞main函数</title>
      <link>https://imroc.io/posts/golang-select/</link>
      <pubDate>Wed, 06 Sep 2017 15:35:18 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/golang-select/</guid>
      <description>很多时候我们需要让main函数不退出，让它在后台一直执行，例如： func main() { for i := 0; i &amp;lt; 20; i++ { //启动20个协程处理消息队列中的消息 c := consumer.New() go c.Start() } select</description>
    </item>
    
  </channel>
</rss>