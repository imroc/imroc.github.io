<!DOCTYPE html><html lang="zh-Hans" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 5.0.0-beta.1 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="roc">

  
  
  
    
  
  <meta name="description" content="
  目录
  
  
    问题描述
    猜测
    抓包
    syn queue 与 accept queue
    listen 与 accept
    Linux 的 backlog
    队列溢出
    回到问题上来
    somaxconn 的默认值很小
      
        方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数
        方式二: 使用 initContainers 设置内核参数
        方式三: 安装 tuning CNI 插件统一设置 sysctl
      
    
    nginx 的 backlog
    参考资料
  



上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。

问题描述
有用户反馈大量图片加载不出来。
图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client &ndash;&gt; k8s ingress &ndash;&gt; nginx &ndash;&gt; nfs。
猜测
猜测: ingress 图片下载路径对应的后端服务出问题了。
验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！
抓包
继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：
# 拿到 pod 中 nginx 的容器 id
$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 &quot;^Containers:&quot; | grep -Eo &#39;docker://.*$&#39; | head -n 1 | sed &#39;s/docker:\/\/\(.*\)$/\1/&#39;
49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e

# 通过容器 id 拿到 nginx 进程 pid
$ docker inspect -f {{.State.Pid}} 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e
3985

# 进入 nginx 进程所在的 netns
$ nsenter -n -t 3985

# 查看容器 netns 中的网卡信息，确认下
$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.26.0.8/26 scope global eth0
       valid_lft forever preferred_lft forever

使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:
tcpdump -i eth0 -nnnn -ttt port 24568

在其它节点准备使用 nc 指定源端口为 24568 向容器发包：
nc -u 24568 172.16.1.21 80

观察抓包结果：
00:00:00.000000 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000206334 ecr 0,nop,wscale 9], length 0
00:00:01.032218 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000207366 ecr 0,nop,wscale 9], length 0
00:00:02.011962 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000209378 ecr 0,nop,wscale 9], length 0
00:00:04.127943 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000213506 ecr 0,nop,wscale 9], length 0
00:00:08.192056 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000221698 ecr 0,nop,wscale 9], length 0
00:00:16.127983 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000237826 ecr 0,nop,wscale 9], length 0
00:00:33.791988 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000271618 ecr 0,nop,wscale 9], length 0

SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。
排除是 iptables 规则问题，在容器 netns 中使用 netstat -s 检查下是否有丢包统计:
$ netstat -s | grep -E &#39;overflow|drop&#39;
    12178939 times the listen queue of a socket overflowed
    12247395 SYNs to LISTEN sockets dropped

果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。">

  
  <link rel="alternate" hreflang="zh-Hans" href="/post/202001/kubernetes-overflow-and-drop/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#1565c0">
  

  
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous" media="print" onload="this.media='all'">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.a2dda909fb8228e17412d06b4423e0a6.css">

  




  

  


  
  

  

  
  <link rel="manifest" href="/index.webmanifest">
  

  <link rel="icon" type="image/png" href="/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/202001/kubernetes-overflow-and-drop/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@imrocchan">
  <meta property="twitter:creator" content="@imrocchan">
  
  <meta property="og:site_name" content="roc的个人网站">
  <meta property="og:url" content="/post/202001/kubernetes-overflow-and-drop/">
  <meta property="og:title" content="Kubernetes 疑难杂症排查分享：神秘的溢出与丢包 | roc的个人网站">
  <meta property="og:description" content="
  目录
  
  
    问题描述
    猜测
    抓包
    syn queue 与 accept queue
    listen 与 accept
    Linux 的 backlog
    队列溢出
    回到问题上来
    somaxconn 的默认值很小
      
        方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数
        方式二: 使用 initContainers 设置内核参数
        方式三: 安装 tuning CNI 插件统一设置 sysctl
      
    
    nginx 的 backlog
    参考资料
  



上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。

问题描述
有用户反馈大量图片加载不出来。
图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client &ndash;&gt; k8s ingress &ndash;&gt; nginx &ndash;&gt; nfs。
猜测
猜测: ingress 图片下载路径对应的后端服务出问题了。
验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！
抓包
继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：
# 拿到 pod 中 nginx 的容器 id
$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 &quot;^Containers:&quot; | grep -Eo &#39;docker://.*$&#39; | head -n 1 | sed &#39;s/docker:\/\/\(.*\)$/\1/&#39;
49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e

# 通过容器 id 拿到 nginx 进程 pid
$ docker inspect -f {{.State.Pid}} 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e
3985

# 进入 nginx 进程所在的 netns
$ nsenter -n -t 3985

# 查看容器 netns 中的网卡信息，确认下
$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.26.0.8/26 scope global eth0
       valid_lft forever preferred_lft forever

使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:
tcpdump -i eth0 -nnnn -ttt port 24568

在其它节点准备使用 nc 指定源端口为 24568 向容器发包：
nc -u 24568 172.16.1.21 80

观察抓包结果：
00:00:00.000000 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000206334 ecr 0,nop,wscale 9], length 0
00:00:01.032218 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000207366 ecr 0,nop,wscale 9], length 0
00:00:02.011962 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000209378 ecr 0,nop,wscale 9], length 0
00:00:04.127943 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000213506 ecr 0,nop,wscale 9], length 0
00:00:08.192056 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000221698 ecr 0,nop,wscale 9], length 0
00:00:16.127983 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000237826 ecr 0,nop,wscale 9], length 0
00:00:33.791988 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000271618 ecr 0,nop,wscale 9], length 0

SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。
排除是 iptables 规则问题，在容器 netns 中使用 netstat -s 检查下是否有丢包统计:
$ netstat -s | grep -E &#39;overflow|drop&#39;
    12178939 times the listen queue of a socket overflowed
    12247395 SYNs to LISTEN sockets dropped

果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。"><meta property="og:image" content="/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="zh-Hans">
  
    
      <meta property="article:published_time" content="2020-01-12T19:20:00&#43;08:00">
    
    <meta property="article:modified_time" content="2020-01-12T19:20:00&#43;08:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/202001/kubernetes-overflow-and-drop/"
  },
  "headline": "Kubernetes 疑难杂症排查分享：神秘的溢出与丢包",
  
  "datePublished": "2020-01-12T19:20:00+08:00",
  "dateModified": "2020-01-12T19:20:00+08:00",
  
  "author": {
    "@type": "Person",
    "name": "roc"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "roc的个人网站",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "\u003cdetails class=\"toc-inpage d-print-none  \" open\u003e\n  \u003csummary class=\"font-weight-bold\"\u003e目录\u003c/summary\u003e\n  \u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#问题描述\"\u003e问题描述\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#猜测\"\u003e猜测\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#抓包\"\u003e抓包\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#syn-queue-与-accept-queue\"\u003esyn queue 与 accept queue\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#listen-与-accept\"\u003elisten 与 accept\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#linux-的-backlog\"\u003eLinux 的 backlog\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#队列溢出\"\u003e队列溢出\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#回到问题上来\"\u003e回到问题上来\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#somaxconn-的默认值很小\"\u003esomaxconn 的默认值很小\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#方式一-使用-k8s-sysctls-特性直接给-pod-指定内核参数\"\u003e方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#方式二-使用-initcontainers-设置内核参数\"\u003e方式二: 使用 initContainers 设置内核参数\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#方式三-安装-tuning-cni-插件统一设置-sysctl\"\u003e方式三: 安装 tuning CNI 插件统一设置 sysctl\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#nginx-的-backlog\"\u003enginx 的 backlog\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#参考资料\"\u003e参考资料\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e\n\u003c/details\u003e\n\u003cblockquote\u003e\n\u003cp\u003e上一篇 \u003ca href=\"https://imroc.io/posts/kubernetes/kubernetes-no-route-to-host/\" target=\"_blank\" rel=\"noopener\"\u003eKubernetes 疑难杂症排查分享: 诡异的 No route to host\u003c/a\u003e 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"问题描述\"\u003e问题描述\u003c/h2\u003e\n\u003cp\u003e有用户反馈大量图片加载不出来。\u003c/p\u003e\n\u003cp\u003e图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client \u0026ndash;\u0026gt; k8s ingress \u0026ndash;\u0026gt; nginx \u0026ndash;\u0026gt; nfs。\u003c/p\u003e\n\u003ch2 id=\"猜测\"\u003e猜测\u003c/h2\u003e\n\u003cp\u003e猜测: ingress 图片下载路径对应的后端服务出问题了。\u003c/p\u003e\n\u003cp\u003e验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！\u003c/p\u003e\n\u003ch2 id=\"抓包\"\u003e抓包\u003c/h2\u003e\n\u003cp\u003e继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e# 拿到 pod 中 nginx 的容器 id\n$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 \u0026quot;^Containers:\u0026quot; | grep -Eo 'docker://.*$' | head -n 1 | sed 's/docker:\\/\\/\\(.*\\)$/\\1/'\n49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e\n\n# 通过容器 id 拿到 nginx 进程 pid\n$ docker inspect -f {{.State.Pid}} 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e\n3985\n\n# 进入 nginx 进程所在的 netns\n$ nsenter -n -t 3985\n\n# 查看容器 netns 中的网卡信息，确认下\n$ ip a\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n3: eth0@if11: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 172.26.0.8/26 scope global eth0\n       valid_lft forever preferred_lft forever\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003etcpdump -i eth0 -nnnn -ttt port 24568\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e在其它节点准备使用 nc 指定源端口为 24568 向容器发包：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003enc -u 24568 172.16.1.21 80\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e观察抓包结果：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e00:00:00.000000 IP 10.0.0.3.24568 \u0026gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000206334 ecr 0,nop,wscale 9], length 0\n00:00:01.032218 IP 10.0.0.3.24568 \u0026gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000207366 ecr 0,nop,wscale 9], length 0\n00:00:02.011962 IP 10.0.0.3.24568 \u0026gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000209378 ecr 0,nop,wscale 9], length 0\n00:00:04.127943 IP 10.0.0.3.24568 \u0026gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000213506 ecr 0,nop,wscale 9], length 0\n00:00:08.192056 IP 10.0.0.3.24568 \u0026gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000221698 ecr 0,nop,wscale 9], length 0\n00:00:16.127983 IP 10.0.0.3.24568 \u0026gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000237826 ecr 0,nop,wscale 9], length 0\n00:00:33.791988 IP 10.0.0.3.24568 \u0026gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000271618 ecr 0,nop,wscale 9], length 0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。\u003c/p\u003e\n\u003cp\u003e排除是 iptables 规则问题，在容器 netns 中使用 \u003ccode\u003enetstat -s\u003c/code\u003e 检查下是否有丢包统计:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e$ netstat -s | grep -E 'overflow|drop'\n    12178939 times the listen queue of a socket overflowed\n    12247395 SYNs to LISTEN sockets dropped\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。\u003c/p\u003e"
}
</script>

  

  


  


  





  <title>Kubernetes 疑难杂症排查分享：神秘的溢出与丢包 | roc的个人网站</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  ">

  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.7264cf0eba3b66951b36da7d2cecf9c5.js"></script>

  

<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>搜索</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="搜索..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="搜索...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="切换导航">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-center" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        

        

        
        
        
        

        
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/"><span>主页</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/post/"><span>技术博客</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#notes"><span>学习笔记</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#tags"><span>内容标签</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="搜索"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>浅色</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>深色</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>自动</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      roc</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jan 12, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 分钟阅读时长
  </span>
  

  
  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <details class="toc-inpage d-print-none  " open>
  <summary class="font-weight-bold">目录</summary>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#问题描述">问题描述</a></li>
    <li><a href="#猜测">猜测</a></li>
    <li><a href="#抓包">抓包</a></li>
    <li><a href="#syn-queue-与-accept-queue">syn queue 与 accept queue</a></li>
    <li><a href="#listen-与-accept">listen 与 accept</a></li>
    <li><a href="#linux-的-backlog">Linux 的 backlog</a></li>
    <li><a href="#队列溢出">队列溢出</a></li>
    <li><a href="#回到问题上来">回到问题上来</a></li>
    <li><a href="#somaxconn-的默认值很小">somaxconn 的默认值很小</a>
      <ul>
        <li><a href="#方式一-使用-k8s-sysctls-特性直接给-pod-指定内核参数">方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数</a></li>
        <li><a href="#方式二-使用-initcontainers-设置内核参数">方式二: 使用 initContainers 设置内核参数</a></li>
        <li><a href="#方式三-安装-tuning-cni-插件统一设置-sysctl">方式三: 安装 tuning CNI 插件统一设置 sysctl</a></li>
      </ul>
    </li>
    <li><a href="#nginx-的-backlog">nginx 的 backlog</a></li>
    <li><a href="#参考资料">参考资料</a></li>
  </ul>
</nav>
</details>
<blockquote>
<p>上一篇 <a href="https://imroc.io/posts/kubernetes/kubernetes-no-route-to-host/" target="_blank" rel="noopener">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a> 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。</p>
</blockquote>
<h2 id="问题描述">问题描述</h2>
<p>有用户反馈大量图片加载不出来。</p>
<p>图片下载走的 k8s ingress，这个 ingress 路径对应后端 service 是一个代理静态图片文件的 nginx deployment，这个 deployment 只有一个副本，静态文件存储在 nfs 上，nginx 通过挂载 nfs 来读取静态文件来提供图片下载服务，所以调用链是：client &ndash;&gt; k8s ingress &ndash;&gt; nginx &ndash;&gt; nfs。</p>
<h2 id="猜测">猜测</h2>
<p>猜测: ingress 图片下载路径对应的后端服务出问题了。</p>
<p>验证：在 k8s 集群直接 curl nginx 的 pod ip，发现不通，果然是后端服务的问题！</p>
<h2 id="抓包">抓包</h2>
<p>继续抓包测试观察，登上 nginx pod 所在节点，进入容器的 netns 中：</p>
<pre><code class="language-bash"># 拿到 pod 中 nginx 的容器 id
$ kubectl describe pod tcpbench-6484d4b457-847gl | grep -A10 &quot;^Containers:&quot; | grep -Eo 'docker://.*$' | head -n 1 | sed 's/docker:\/\/\(.*\)$/\1/'
49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e

# 通过容器 id 拿到 nginx 进程 pid
$ docker inspect -f {{.State.Pid}} 49b4135534dae77ce5151c6c7db4d528f05b69b0c6f8b9dd037ec4e7043c113e
3985

# 进入 nginx 进程所在的 netns
$ nsenter -n -t 3985

# 查看容器 netns 中的网卡信息，确认下
$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
3: eth0@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether 56:04:c7:28:b0:3c brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.26.0.8/26 scope global eth0
       valid_lft forever preferred_lft forever
</code></pre>
<p>使用 tcpdump 指定端口 24568 抓容器 netns 中 eth0 网卡的包:</p>
<pre><code class="language-bash">tcpdump -i eth0 -nnnn -ttt port 24568
</code></pre>
<p>在其它节点准备使用 nc 指定源端口为 24568 向容器发包：</p>
<pre><code class="language-bash">nc -u 24568 172.16.1.21 80
</code></pre>
<p>观察抓包结果：</p>
<pre><code class="language-bash">00:00:00.000000 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000206334 ecr 0,nop,wscale 9], length 0
00:00:01.032218 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000207366 ecr 0,nop,wscale 9], length 0
00:00:02.011962 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000209378 ecr 0,nop,wscale 9], length 0
00:00:04.127943 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000213506 ecr 0,nop,wscale 9], length 0
00:00:08.192056 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000221698 ecr 0,nop,wscale 9], length 0
00:00:16.127983 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000237826 ecr 0,nop,wscale 9], length 0
00:00:33.791988 IP 10.0.0.3.24568 &gt; 172.16.1.21.80: Flags [S], seq 416500297, win 29200, options [mss 1424,sackOK,TS val 3000271618 ecr 0,nop,wscale 9], length 0
</code></pre>
<p>SYN 包到容器内网卡了，但容器没回 ACK，像是报文到达容器内的网卡后就被丢了。看样子跟防火墙应该也没什么关系，也检查了容器 netns 内的 iptables 规则，是空的，没问题。</p>
<p>排除是 iptables 规则问题，在容器 netns 中使用 <code>netstat -s</code> 检查下是否有丢包统计:</p>
<pre><code class="language-bash">$ netstat -s | grep -E 'overflow|drop'
    12178939 times the listen queue of a socket overflowed
    12247395 SYNs to LISTEN sockets dropped
</code></pre>
<p>果然有丢包，为了理解这里的丢包统计，我深入研究了一下，下面插播一些相关知识。</p>
<h2 id="syn-queue-与-accept-queue">syn queue 与 accept queue</h2>
<p>Linux 进程监听端口时，内核会给它对应的 socket 分配两个队列：</p>
<ul>
<li>syn queue: 半连接队列。server 收到 SYN 后，连接会先进入 <code>SYN_RCVD</code> 状态，并放入 syn queue，此队列的包对应还没有完全建立好的连接（TCP 三次握手还没完成）。</li>
<li>accept queue: 全连接队列。当 TCP 三次握手完成之后，连接会进入 <code>ESTABELISHED</code> 状态并从 syn queue 移到 accept queue，等待被进程调用 <code>accept()</code> 系统调用 &ldquo;拿走&rdquo;。</li>
</ul>
<blockquote>
<p>注意：这两个队列的连接都还没有真正被应用层接收到，当进程调用 <code>accept()</code> 后，连接才会被应用层处理，具体到我们这个问题的场景就是 nginx 处理 HTTP 请求。</p>
</blockquote>
<p>为了更好理解，可以看下这张 TCP 连接建立过程的示意图：</p>
<p><img src="https://imroc.io/assets/blog/troubleshooting-k8s-network/backlog.png" alt=""></p>
<h2 id="listen-与-accept">listen 与 accept</h2>
<p>不管使用什么语言和框架，在写 server 端应用时，它们的底层在监听端口时最终都会调用 <code>listen()</code> 系统调用，处理新请求时都会先调用 <code>accept()</code> 系统调用来获取新的连接，然后再处理请求，只是有各自不同的封装而已，以 go 语言为例：</p>
<pre><code class="language-go">// 调用 listen 监听端口
l, err := net.Listen(&quot;tcp&quot;, &quot;:80&quot;)
if err != nil {
	panic(err)
}
for {
	// 不断调用 accept 获取新连接，如果 accept queue 为空就一直阻塞
	conn, err := l.Accept()
	if err != nil {
		log.Println(&quot;accept error:&quot;, err)
		continue
    }
	// 每来一个新连接意味着一个新请求，启动协程处理请求
	go handle(conn)
}
</code></pre>
<h2 id="linux-的-backlog">Linux 的 backlog</h2>
<p>内核既然给监听端口的 socket 分配了 syn queue 与 accept queue 两个队列，那它们有大小限制吗？可以无限往里面塞数据吗？当然不行！ 资源是有限的，尤其是在内核态，所以需要限制一下这两个队列的大小。那么它们的大小是如何确定的呢？我们先来看下 listen 这个系统调用:</p>
<pre><code class="language-bash">int listen(int sockfd, int backlog)
</code></pre>
<p>可以看到，能够传入一个整数类型的 <code>backlog</code> 参数，我们再通过 <code>man listen</code> 看下解释：</p>
<p><code>The behavior of the backlog argument on TCP sockets changed with Linux 2.2.  Now it specifies the queue length for completely established sockets waiting to  be  accepted,  instead  of  the  number  of  incomplete  connection requests.   The  maximum  length  of  the queue for incomplete sockets can be set using /proc/sys/net/ipv4/tcp_max_syn_backlog.  When syncookies are enabled there is no logical maximum length and this setting is ignored.  See tcp(7) for more information. </code></p>
<p><code>If the backlog argument is greater than the value in /proc/sys/net/core/somaxconn, then it is silently truncated to that value; the default value in this file is 128.  In kernels before 2.4.25, this limit  was  a  hard  coded value, SOMAXCONN, with the value 128.</code></p>
<p>继续深挖了一下源码，结合这里的解释提炼一下：</p>
<ul>
<li>listen 的 backlog 参数同时指定了 socket 的 syn queue 与 accept queue 大小。</li>
<li>accept queue 最大不能超过 <code>net.core.somaxconn</code> 的值，即:
<pre><code>max accept queue size = min(backlog, net.core.somaxconn)
</code></pre>
</li>
<li>如果启用了 syncookies (net.ipv4.tcp_syncookies=1)，当 syn queue 满了，server 还是可以继续接收 <code>SYN</code> 包并回复 <code>SYN+ACK</code> 给 client，只是不会存入 syn queue 了。因为会利用一套巧妙的 syncookies 算法机制生成隐藏信息写入响应的 <code>SYN+ACK</code> 包中，等 client 回 <code>ACK</code> 时，server 再利用 syncookies 算法校验报文，校验通过后三次握手就顺利完成了。所以如果启用了 syncookies，syn queue 的逻辑大小是没有限制的，</li>
<li>syncookies 通常都是启用了的，所以一般不用担心 syn queue 满了导致丢包。syncookies 是为了防止 SYN Flood 攻击 (一种常见的 DDoS 方式)，攻击原理就是 client 不断发 SYN 包但不回最后的 ACK，填满 server 的 syn queue 从而无法建立新连接，导致 server 拒绝服务。</li>
<li>如果 syncookies 没有启用，syn queue 的大小就有限制，除了跟 accept queue 一样受 <code>net.core.somaxconn</code> 大小限制之外，还会受到 <code>net.ipv4.tcp_max_syn_backlog</code> 的限制，即:
<pre><code>max syn queue size = min(backlog, net.core.somaxconn, net.ipv4.tcp_max_syn_backlog)
</code></pre>
</li>
</ul>
<p>4.3 及其之前版本的内核，syn queue 的大小计算方式跟现在新版内核这里还不一样，详细请参考 commit <a href="https://github.com/torvalds/linux/commit/ef547f2ac16bd9d77a780a0e7c70857e69e8f23f#diff-56ecfd3cd70d57cde321f395f0d8d743L43" target="_blank" rel="noopener">ef547f2ac16b</a></p>
<h2 id="队列溢出">队列溢出</h2>
<p>毫无疑问，在队列大小有限制的情况下，如果队列满了，再有新连接过来肯定就有问题。</p>
<p>翻下 linux 源码，看下处理 SYN 包的部分，在 <code>net/ipv4/tcp_input.c</code> 的 <code>tcp_conn_request</code> 函数:</p>
<pre><code class="language-c">if ((net-&gt;ipv4.sysctl_tcp_syncookies == 2 ||
     inet_csk_reqsk_queue_is_full(sk)) &amp;&amp; !isn) {
	want_cookie = tcp_syn_flood_action(sk, rsk_ops-&gt;slab_name);
	if (!want_cookie)
		goto drop;
}

if (sk_acceptq_is_full(sk)) {
	NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
	goto drop;
}
</code></pre>
<p><code>goto drop</code> 最终会走到 <code>tcp_listendrop</code> 函数，实际上就是将 <code>ListenDrops</code> 计数器 +1:</p>
<pre><code class="language-c">static inline void tcp_listendrop(const struct sock *sk)
{
	atomic_inc(&amp;((struct sock *)sk)-&gt;sk_drops);
	__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENDROPS);
}
</code></pre>
<p>大致可以看出来，对于 SYN 包：</p>
<ul>
<li>如果 syn queue 满了并且没有开启 syncookies 就丢包，并将 <code>ListenDrops</code> 计数器 +1。</li>
<li>如果 accept queue 满了也会丢包，并将 <code>ListenOverflows</code> 和 <code>ListenDrops</code> 计数器 +1。</li>
</ul>
<p>而我们前面排查问题通过 <code>netstat -s</code> 看到的丢包统计，其实就是对应的 <code>ListenOverflows</code> 和 <code>ListenDrops</code> 这两个计数器。</p>
<p>除了用 <code>netstat -s</code>，还可以使用 <code>nstat -az</code> 直接看系统内各个计数器的值:</p>
<pre><code class="language-bash">$ nstat -az | grep -E 'TcpExtListenOverflows|TcpExtListenDrops'
TcpExtListenOverflows           12178939              0.0
TcpExtListenDrops               12247395              0.0
</code></pre>
<p>另外，对于低版本内核，当 accept queue 满了，并不会完全丢弃 SYN 包，而是对 SYN 限速。把内核源码切到 3.10 版本，看 <code>net/ipv4/tcp_ipv4.c</code> 中 <code>tcp_v4_conn_request</code> 函数:</p>
<pre><code class="language-c">/* Accept backlog is full. If we have already queued enough
 * of warm entries in syn queue, drop request. It is better than
 * clogging syn queue with openreqs with exponentially increasing
 * timeout.
 */
if (sk_acceptq_is_full(sk) &amp;&amp; inet_csk_reqsk_queue_young(sk) &gt; 1) {
        NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
        goto drop;
}
</code></pre>
<p>其中 <code>inet_csk_reqsk_queue_young(sk) &gt; 1</code> 的条件实际就是用于限速，仿佛在对 client 说: 哥们，你慢点！我的 accept queue 都满了，即便咱们握手成功，连接也可能放不进去呀。</p>
<h2 id="回到问题上来">回到问题上来</h2>
<p>总结之前观察到两个现象：</p>
<ul>
<li>容器内抓包发现收到 client 的 SYN，但 nginx 没回包。</li>
<li>通过 <code>netstat -s</code> 发现有溢出和丢包的统计 (<code>ListenOverflows</code> 与 <code>ListenDrops</code>)。</li>
</ul>
<p>根据之前的分析，我们可以推测是 syn queue 或 accept queue 满了。</p>
<p>先检查下 syncookies 配置:</p>
<pre><code class="language-bash">$ cat /proc/sys/net/ipv4/tcp_syncookies
1
</code></pre>
<p>确认启用了 <code>syncookies</code>，所以 syn queue 大小没有限制，不会因为 syn queue 满而丢包，并且即便没开启 <code>syncookies</code>，syn queue 有大小限制，队列满了也不会使 <code>ListenOverflows</code> 计数器 +1。</p>
<p>从计数器结果来看，<code>ListenOverflows</code> 和 <code>ListenDrops</code> 的值差别不大，所以推测很有可能是 accept queue 满了，因为当 accept queue 满了会丢 SYN 包，并且同时将 <code>ListenOverflows</code> 与 <code>ListenDrops</code> 计数器分别 +1。</p>
<p>如何验证 accept queue 满了呢？可以在容器的 netns 中执行 <code>ss -lnt</code> 看下:</p>
<pre><code class="language-bash">$ ss -lnt
State      Recv-Q Send-Q Local Address:Port                Peer Address:Port
LISTEN     129    128                *:80                             *:*
</code></pre>
<p>通过这条命令我们可以看到当前 netns 中监听 tcp 80 端口的 socket，<code>Send-Q</code> 为 128，<code>Recv-Q</code> 为 129。</p>
<p>什么意思呢？通过调研得知：</p>
<ul>
<li>对于 <code>LISTEN</code> 状态，<code>Send-Q</code> 表示 accept queue 的最大限制大小，<code>Recv-Q</code> 表示其实际大小。</li>
<li>对于 <code>ESTABELISHED</code> 状态，<code>Send-Q</code> 和 <code>Recv-Q</code> 分别表示发送和接收数据包的 buffer。</li>
</ul>
<p>所以，看这里输出结果可以得知 accept queue 满了，当 <code>Recv-Q</code> 的值比 <code>Send-Q</code> 大 1 时表明 accept queue 溢出了，如果再收到 SYN 包就会丢弃掉。</p>
<p>导致 accept queue 满的原因一般都是因为进程调用 <code>accept()</code> 太慢了，导致大量连接不能被及时 &ldquo;拿走&rdquo;。</p>
<p>那么什么情况下进程调用 <code>accept()</code> 会很慢呢？猜测可能是进程连接负载高，处理不过来。</p>
<p>而负载高不仅可能是 CPU 繁忙导致，还可能是 IO 慢导致，当文件 IO 慢时就会有很多 IO WAIT，在 IO WAIT 时虽然 CPU 不怎么干活，但也会占据 CPU 时间片，影响 CPU 干其它活。</p>
<p>最终进一步定位发现是 nginx pod 挂载的 nfs 服务对应的 nfs server 负载较高，导致 IO 延时较大，从而使 nginx 调用 <code>accept()</code> 变慢，accept queue 溢出，使得大量代理静态图片文件的请求被丢弃，也就导致很多图片加载不出来。</p>
<p>虽然根因不是 k8s 导致的问题，但也从中挖出一些在高并发场景下值得优化的点，请继续往下看。</p>
<h2 id="somaxconn-的默认值很小">somaxconn 的默认值很小</h2>
<p>我们再看下之前 <code>ss -lnt</code> 的输出:</p>
<pre><code class="language-bash">$ ss -lnt
State      Recv-Q Send-Q Local Address:Port                Peer Address:Port
LISTEN     129    128                *:80                             *:*
</code></pre>
<p>仔细一看，<code>Send-Q</code> 表示 accept queue 最大的大小，才 128 ？也太小了吧！</p>
<p>根据前面的介绍我们知道，accept queue 的最大大小会受 <code>net.core.somaxconn</code> 内核参数的限制，我们看下 pod 所在节点上这个内核参数的大小:</p>
<pre><code class="language-bash">$ cat /proc/sys/net/core/somaxconn
32768
</code></pre>
<p>是 32768，挺大的，为什么这里 accept queue 最大大小就只有 128 了呢？</p>
<p><code>net.core.somaxconn</code> 这个内核参数是 namespace 隔离了的，我们在容器 netns 中再确认了下：</p>
<pre><code class="language-bash">$ cat /proc/sys/net/core/somaxconn
128
</code></pre>
<p>为什么只有 128？看下 stackoverflow <a href="https://stackoverflow.com/questions/26177059/refresh-net-core-somaxcomm-or-any-sysctl-property-for-docker-containers/26197875#26197875" target="_blank" rel="noopener">这里</a> 的讨论:</p>
<p><code>The &quot;net/core&quot; subsys is registered per network namespace. And the initial value for somaxconn is set to 128.</code></p>
<p>原来新建的 netns 中 somaxconn 默认就为 128，在 <code>include/linux/socket.h</code> 中可以看到这个常量的定义:</p>
<pre><code class="language-c">/* Maximum queue length specifiable by listen.  */
#define SOMAXCONN	128
</code></pre>
<p>很多人在使用 k8s 时都没太在意这个参数，为什么大家平常在较高并发下也没发现有问题呢？</p>
<p>因为通常进程 <code>accept()</code> 都是很快的，所以一般 accept queue 基本都没什么积压的数据，也就不会溢出导致丢包了。</p>
<p>对于并发量很高的应用，还是建议将 somaxconn 调高。虽然可以进入容器 netns 后使用 <code>sysctl -w net.core.somaxconn=1024</code> 或 <code>echo 1024 &gt; /proc/sys/net/core/somaxconn</code> 临时调整，但调整的意义不大，因为容器内的进程一般在启动的时候才会调用 <code>listen()</code>，然后 accept queue 的大小就被决定了，并且不再改变。</p>
<p>下面介绍几种调整方式:</p>
<h3 id="方式一-使用-k8s-sysctls-特性直接给-pod-指定内核参数">方式一: 使用 k8s sysctls 特性直接给 pod 指定内核参数</h3>
<p>示例 yaml:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example
spec:
  securityContext:
    sysctls:
    - name: net.core.somaxconn
      value: &quot;8096&quot;
</code></pre>
<p>有些参数是 <code>unsafe</code> 类型的，不同环境不一样，我的环境里是可以直接设置 pod 的 <code>net.core.somaxconn</code> 这个 sysctl 的。如果你的环境不行，请参考官方文档 <a href="https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/#enabling-unsafe-sysctls" target="_blank" rel="noopener">Using sysctls in a Kubernetes Cluster</a> 启用 <code>unsafe</code> 类型的 sysctl。</p>
<blockquote>
<p>注：此特性在 k8s v1.12 beta，默认开启。</p>
</blockquote>
<h3 id="方式二-使用-initcontainers-设置内核参数">方式二: 使用 initContainers 设置内核参数</h3>
<p>示例 yaml:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sysctl-example-init
spec:
  initContainers:
  - image: busybox
    command:
    - sh
    - -c
    - echo 1024 &gt; /proc/sys/net/core/somaxconn
    imagePullPolicy: Always
    name: setsysctl
    securityContext:
      privileged: true
  Containers:
  ...
</code></pre>
<blockquote>
<p>注: init container 需要 privileged 权限。</p>
</blockquote>
<h3 id="方式三-安装-tuning-cni-插件统一设置-sysctl">方式三: 安装 tuning CNI 插件统一设置 sysctl</h3>
<p>tuning plugin 地址: <a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/tuning">https://github.com/containernetworking/plugins/tree/master/plugins/meta/tuning</a></p>
<p>CNI 配置示例:</p>
<pre><code class="language-bash">{
  &quot;name&quot;: &quot;mytuning&quot;,
  &quot;type&quot;: &quot;tuning&quot;,
  &quot;sysctl&quot;: {
          &quot;net.core.somaxconn&quot;: &quot;1024&quot;
  }
}
</code></pre>
<h2 id="nginx-的-backlog">nginx 的 backlog</h2>
<p>我们使用方式一尝试给 nginx pod 的 somaxconn 调高到 8096 后观察:</p>
<pre><code class="language-bash">$ ss -lnt
State      Recv-Q Send-Q Local Address:Port                Peer Address:Port
LISTEN     512    511                *:80                             *:*
</code></pre>
<p>WTF? 还是溢出了，而且调高了 somaxconn 之后虽然 accept queue 的最大大小 (<code>Send-Q</code>) 变大了，但跟 8096 还差很远呀！</p>
<p>在经过一番研究，发现 nginx 在 <code>listen()</code> 时并没有读取 somaxconn 作为 backlog 默认值传入，它有自己的默认值，也支持在配置里改。通过 <a href="http://nginx.org/en/docs/http/ngx_http_core_module.html" target="_blank" rel="noopener">ngx_http_core_module</a> 的官方文档我们可以看到它在 linux 下的默认值就是 511:</p>
<pre><code>backlog=number
   sets the backlog parameter in the listen() call that limits the maximum length for the queue of pending connections. By default, backlog is set to -1 on FreeBSD, DragonFly BSD, and macOS, and to 511 on other platforms.
</code></pre>
<p>配置示例:</p>
<pre><code class="language-bash">listen  80  default  backlog=1024;
</code></pre>
<p>所以，在容器中使用 nginx 来支撑高并发的业务时，记得要同时调整下 <code>net.core.somaxconn</code> 内核参数和 <code>nginx.conf</code> 中的 backlog 配置。</p>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>Using sysctls in a Kubernetes Cluster: <a href="https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/">https://kubernetes-io-vnext-staging.netlify.com/docs/tasks/administer-cluster/sysctl-cluster/</a></li>
<li>SYN packet handling in the wild: <a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/">https://blog.cloudflare.com/syn-packet-handling-in-the-wild/</a></li>
</ul>
    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/kubernetes/">kubernetes</a>
  
  <a class="badge badge-light" href="/tag/network/">network</a>
  
  <a class="badge badge-light" href="/tag/troubleshooting/">troubleshooting</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/202001/kubernetes-overflow-and-drop/&amp;text=Kubernetes%20%e7%96%91%e9%9a%be%e6%9d%82%e7%97%87%e6%8e%92%e6%9f%a5%e5%88%86%e4%ba%ab%ef%bc%9a%e7%a5%9e%e7%a7%98%e7%9a%84%e6%ba%a2%e5%87%ba%e4%b8%8e%e4%b8%a2%e5%8c%85" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/202001/kubernetes-overflow-and-drop/&amp;t=Kubernetes%20%e7%96%91%e9%9a%be%e6%9d%82%e7%97%87%e6%8e%92%e6%9f%a5%e5%88%86%e4%ba%ab%ef%bc%9a%e7%a5%9e%e7%a7%98%e7%9a%84%e6%ba%a2%e5%87%ba%e4%b8%8e%e4%b8%a2%e5%8c%85" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Kubernetes%20%e7%96%91%e9%9a%be%e6%9d%82%e7%97%87%e6%8e%92%e6%9f%a5%e5%88%86%e4%ba%ab%ef%bc%9a%e7%a5%9e%e7%a7%98%e7%9a%84%e6%ba%a2%e5%87%ba%e4%b8%8e%e4%b8%a2%e5%8c%85&amp;body=/post/202001/kubernetes-overflow-and-drop/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/202001/kubernetes-overflow-and-drop/&amp;title=Kubernetes%20%e7%96%91%e9%9a%be%e6%9d%82%e7%97%87%e6%8e%92%e6%9f%a5%e5%88%86%e4%ba%ab%ef%bc%9a%e7%a5%9e%e7%a7%98%e7%9a%84%e6%ba%a2%e5%87%ba%e4%b8%8e%e4%b8%a2%e5%8c%85" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Kubernetes%20%e7%96%91%e9%9a%be%e6%9d%82%e7%97%87%e6%8e%92%e6%9f%a5%e5%88%86%e4%ba%ab%ef%bc%9a%e7%a5%9e%e7%a7%98%e7%9a%84%e6%ba%a2%e5%87%ba%e4%b8%8e%e4%b8%a2%e5%8c%85%20/post/202001/kubernetes-overflow-and-drop/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/202001/kubernetes-overflow-and-drop/&amp;title=Kubernetes%20%e7%96%91%e9%9a%be%e6%9d%82%e7%97%87%e6%8e%92%e6%9f%a5%e5%88%86%e4%ba%ab%ef%bc%9a%e7%a5%9e%e7%a7%98%e7%9a%84%e6%ba%a2%e5%87%ba%e4%b8%8e%e4%b8%a2%e5%8c%85" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/"><img class="avatar mr-3 avatar-circle" src="/author/roc/avatar_hu49c03114a9ce8ad2bc0bd62c0ddb3f2a_64404_270x270_fill_q75_lanczos_center.jpg" alt="roc"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">roc</a></h5>
      <h6 class="card-subtitle">腾讯云高级工程师</h6>
      <p class="card-text">授人以鱼不如授人以渔</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/imroc" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:roc@imroc.io" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/imrocchan" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/%E9%B9%8F-%E9%99%88-b51646113" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.youtube.com/channel/UCEtWQhtiNi1vvDmoC6wydNQ" target="_blank" rel="noopener">
        <i class="fab fa-youtube"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  










<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">下一页</div>
    <a href="/post/202003/build-cloud-native-large-scale-distributed-monitoring-system-1/" rel="next">打造云原生大型分布式监控系统(一): 大规模场景下 Prometheus 的优化手段</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">上一页</div>
    <a href="/post/201912/kubernetes-no-route-to-host/" rel="prev">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a>
  </div>
  
</div>

</div>





  
  
  <div class="article-widget content-widget-hr">
    <h3>相关</h3>
    <ul>
      
      <li><a href="/post/201912/kubernetes-no-route-to-host/">Kubernetes 疑难杂症排查分享: 诡异的 No route to host</a></li>
      
      <li><a href="/post/201908/troubleshooting-with-kubernetes-network/">Kubernetes 网络疑难杂症排查分享</a></li>
      
      <li><a href="/post/202011/build-cloud-native-large-scale-distributed-monitoring-system-4/">打造云原生大型分布式监控系统(四): Kvass&#43;Thanos 监控超大规模容器集群</a></li>
      
      <li><a href="/post/202009/nginx-ingress-high-concurrency/">Nginx Ingress 高并发实践</a></li>
      
      <li><a href="/post/202008/nginx-ingress-on-tke/">Nginx Ingress on TKE 部署最佳实践</a></li>
      
    </ul>
  </div>
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">
  

  <p class="powered-by">
    roc © 2021
  </p>

  
  






  <p class="powered-by">
    
    
    
    Published with
    <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">引用</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> 复制
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> 下载
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
      

    

    
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
        <div class="search-hit-content">
          <div class="search-hit-name">
            <a href="{{relpermalink}}">{{title}}</a>
            <div class="article-metadata search-hit-type">{{type}}</div>
            <p class="search-hit-description">{{snippet}}</p>
          </div>
        </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    
    

    
    
    

    
    

    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/zh/js/wowchemy.min.64fd16cc68b6a5f25e0156dfff88edff.js"></script>

    






</body>
</html>
