<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on roc</title>
    <link>https://imroc.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on roc</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <managingEditor>roc@imroc.io (roc)</managingEditor>
    <webMaster>roc@imroc.io (roc)</webMaster>
    <lastBuildDate>Sun, 12 Jan 2020 19:20:00 +0800</lastBuildDate>
    
	<atom:link href="https://imroc.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kubernetes 疑难杂症排查分享：神秘的溢出与丢包</title>
      <link>https://imroc.io/posts/kubernetes-overflow-and-drop/</link>
      <pubDate>Sun, 12 Jan 2020 19:20:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-overflow-and-drop/</guid>
      <description>上一篇 Kubernetes 疑难杂症排查分享: 诡异的 No route to host 不小心又爆火，这次继续带来干货，看之前请提前泡好茶，避免口干。 问题描述 有用户反馈大量图片加载不出来。</description>
    </item>
    
    <item>
      <title>Kubernetes 疑难杂症排查分享: 诡异的 No route to host</title>
      <link>https://imroc.io/posts/kubernetes-no-route-to-host/</link>
      <pubDate>Sun, 15 Dec 2019 12:03:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-no-route-to-host/</guid>
      <description>&lt;p&gt;之前发过一篇干货满满的爆火文章 &lt;a href=&#34;https://imroc.io/posts/kubernetes/troubleshooting-with-kubernetes-network/&#34;&gt;Kubernetes 网络疑难杂症排查分享&lt;/a&gt;，包含多个疑难杂症的排查案例分享，信息量巨大。这次我又带来了续集，只讲一个案例，但信息量也不小，Are you ready ?&lt;/p&gt;
&lt;h2 id=&#34;heading&#34;&gt;问题反馈&lt;/h2&gt;
&lt;p&gt;有用户反馈 Deployment 滚动更新的时候，业务日志偶尔会报 &amp;ldquo;No route to host&amp;rdquo; 的错误。&lt;/p&gt;
&lt;h2 id=&#34;heading-1&#34;&gt;分析&lt;/h2&gt;
&lt;p&gt;之前没遇到滚动更新会报 &amp;ldquo;No route to host&amp;rdquo; 的问题，我们先看下滚动更新导致连接异常有哪些常见的报错:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connection reset by peer&lt;/code&gt;: 连接被重置。通常是连接建立过，但 server 端发现 client 发的包不对劲就返回 RST，应用层就报错连接被重置。比如在 server 滚动更新过程中，client 给 server 发的请求还没完全结束，或者本身是一个类似 grpc 的多路复用长连接，当 server 对应的旧 Pod 删除(没有做优雅结束，停止时没有关闭连接)，新 Pod 很快创建启动并且刚好有跟之前旧 Pod 一样的 IP，这时 kube-proxy 也没感知到这个 IP 其实已经被删除然后又被重建了，针对这个 IP 的规则就不会更新，旧的连接依然发往这个 IP，但旧 Pod 已经不在了，后面继续发包时依然转发给这个 Pod IP，最终会被转发到这个有相同 IP 的新 Pod 上，而新 Pod 收到此包时检查报文发现不对劲，就返回 RST 给 client 告知将连接重置。针对这种情况，建议应用自身处理好优雅结束：Pod 进入 Terminating 状态后会发送 &lt;code&gt;SIGTERM&lt;/code&gt; 信号给业务进程，业务进程的代码需处理这个信号，在进程退出前关闭所有连接。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connection refused&lt;/code&gt;: 连接被拒绝。通常是连接还没建立，client 正在发 SYN 包请求建立连接，但到了 server 之后发现端口没监听，内核就返回 RST 包，然后应用层就报错连接被拒绝。比如在 server 滚动更新过程中，旧的 Pod 中的进程很快就停止了(网卡还未完全销毁)，但 client 所在节点的 iptables/ipvs 规则还没更新，包就可能会被转发到了这个停止的 Pod (由于 k8s 的 controller 模式，从 Pod 删除到 service 的 endpoint 更新，再到 kube-proxy watch 到更新并更新 节点上的 iptables/ipvs 规则，这个过程是异步的，中间存在一点时间差，所以有可能存在 Pod 中的进程已经没有监听，但 iptables/ipvs 规则还没更新的情况)。针对这种情况，建议给容器加一个 preStop，在真正销毁 Pod 之前等待一段时间，留时间给 kube-proxy 更新转发规则，更新完之后就不会再有新连接往这个旧 Pod 转发了，preStop 示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;lifecycle:
  preStop:
    exec:
      command:
      - /bin/bash
      - -c
      - sleep &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;另外，还可能是新的 Pod 启动比较慢，虽然状态已经 Ready，但实际上可能端口还没监听，新的请求被转发到这个还没完全启动的 Pod 就会报错连接被拒绝。针对这种情况，建议给容器加就绪检查 (readinessProbe)，让容器真正启动完之后才将其状态置为 Ready，然后 kube-proxy 才会更新转发规则，这样就能保证新的请求只被转发到完全启动的 Pod，readinessProbe 示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;readinessProbe:
  httpGet:
    path: /healthz
    port: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    httpHeaders:
    - name: X-Custom-Header
      value: Awesome
  initialDelaySeconds: &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
  timeoutSeconds: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Connection timed out&lt;/code&gt;: 连接超时。通常是连接还没建立，client 发 SYN 请求建立连接一直等到超时时间都没有收到 ACK，然后就报错连接超时。这个可能场景跟前面 &lt;code&gt;Connection refused&lt;/code&gt; 可能的场景类似，不同点在于端口有监听，但进程无法正常响应了: 转发规则还没更新，旧 Pod 的进程正在停止过程中，虽然端口有监听，但已经不响应了；或者转发规则更新了，新 Pod 端口也监听了，但还没有真正就绪，还没有能力处理新请求。针对这些情况的建议跟前面一样：加 preStop 和 readinessProbe。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面我们来继续分析下滚动更新时发生 &lt;code&gt;No route to host&lt;/code&gt; 的可能情况。&lt;/p&gt;
&lt;p&gt;这个报错很明显，IP 无法路由，通常是将报文发到了一个已经彻底销毁的 Pod (网卡已经不在)。不可能发到一个网卡还没创建好的 Pod，因为即便不加存活检查，也是要等到 Pod 网络初始化完后才可能 Ready，然后 kube-proxy 才会更新转发规则。&lt;/p&gt;
&lt;p&gt;什么情况下会转发到一个已经彻底销毁的 Pod？ 借鉴前面几种滚动更新的报错分析，我们推测应该是 Pod 很快销毁了但转发规则还没更新，从而新的请求被转发了这个已经销毁的 Pod，最终报文到达这个 Pod 所在 PodCIDR 的 Node 上时，Node 发现本机已经没有这个 IP 的容器，然后 Node 就返回 ICMP 包告知 client 这个 IP 不可达，client 收到 ICMP 后，应用层就会报错 &amp;ldquo;No route to host&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;所以根据我们的分析，关键点在于 Pod 销毁太快，转发规则还没来得及更新，导致后来的请求被转发到已销毁的 Pod。针对这种情况，我们可以给容器加一个 preStop，留时间给 kube-proxy 更新转发规则来解决，参考 《Kubernetes实践指南》中的部分章节: &lt;a href=&#34;https://k8s.imroc.io/best-practice/high-availability-deployment-of-applications#smooth-update-using-prestophook-and-readinessprobe&#34;&gt;https://k8s.imroc.io/best-practice/high-availability-deployment-of-applications#smooth-update-using-prestophook-and-readinessprobe&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k8s v1.17 新特性预告: 拓扑感知服务路由</title>
      <link>https://imroc.io/posts/kubernetes-service-topology/</link>
      <pubDate>Tue, 26 Nov 2019 16:49:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-service-topology/</guid>
      <description>&lt;p&gt;今天给大家介绍下我参与开发的一个 k8s v1.17 新特性: 拓扑感知服务路由。&lt;/p&gt;
&lt;h2 id=&#34;heading&#34;&gt;名词解释&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;拓扑域: 表示在集群中的某一类 &amp;ldquo;地方&amp;rdquo;，比如某节点、某机架、某可用区或某地域等，这些都可以作为某种拓扑域。&lt;/li&gt;
&lt;li&gt;endpoint: k8s 某个服务的某个 ip+port，通常是 pod 的 ip+port。&lt;/li&gt;
&lt;li&gt;service: k8s 的 service 资源(服务)，关联一组 endpoint ，访问 service 会被转发到关联的某个 endpoint 上。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;heading-1&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;拓扑感知服务路由，此特性最初由杜军大佬提出并设计。为什么要设计此特性呢？想象一下，k8s 集群节点分布在不同的地方，service 对应的 endpoints 分布在不同节点，传统转发策略会对所有 endpoint 做负载均衡，通常会等概率转发，当访问 service 时，流量就可能被分散打到这些不同的地方。虽然 service 转发做了负载均衡，但如果 endpoint 距离比较远，流量转发过去网络时延就相对比较高，会影响网络性能，在某些情况下甚至还可能会付出额外的流量费用。要是如能实现 service 就近转发 endpoint，是不是就可以实现降低网络时延，提升网络性能了呢？是的！这也正是该特性所提出的目的和意义。&lt;/p&gt;
&lt;h2 id=&#34;k8s-&#34;&gt;k8s 亲和性&lt;/h2&gt;
&lt;p&gt;service 的就近转发实际就是一种网络的亲和性，倾向于转发到离自己比较近的 endpoint。在此特性之前，已经在调度和存储方面有一些亲和性的设计与实现:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点亲和性 (Node Affinity): 让 Pod 被调度到符合一些期望条件的 Node 上，比如限制调度到某一可用区，或者要求节点支持 GPU，这算是调度亲和，调度结果取决于节点属性。&lt;/li&gt;
&lt;li&gt;Pod 亲和性与反亲和性 (Pod Affinity/AntiAffinity): 让一组 Pod 调度到同一拓扑域的节点上，或者打散到不同拓扑域的节点， 这也算是调度亲和，调度结果取决于其它 Pod。&lt;/li&gt;
&lt;li&gt;数据卷拓扑感知调度 (Volume Topology-aware Scheduling): 让 Pod 只被调度到符合其绑定的存储所在拓扑域的节点上，这算是调度与存储的亲和，调度结果取决于存储的拓扑域。&lt;/li&gt;
&lt;li&gt;本地数据卷 (Local Persistent Volume): 让 Pod 使用本地数据卷，比如高性能 SSD，在某些需要高 IOPS 低时延的场景很有用，它还会保证 Pod 始终被调度到同一节点，数据就不会不丢失，这也算是调度与存储的亲和，调度结果取决于存储所在节点。&lt;/li&gt;
&lt;li&gt;数据卷拓扑感知动态创建 (Topology-Aware Volume Dynamic Provisioning): 先调度 Pod，再根据 Pod 所在节点的拓扑域来创建存储，这算是存储与调度的亲和，存储的创建取决于调度的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而 k8s 目前在网络方面还没有亲和性能力，拓扑感知服务路由这个新特性恰好可以补齐这个的空缺，此特性使得 service 可以实现就近转发而不是所有 endpoint 等概率转发。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 网络疑难杂症排查分享</title>
      <link>https://imroc.io/posts/troubleshooting-with-kubernetes-network/</link>
      <pubDate>Mon, 12 Aug 2019 16:59:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/troubleshooting-with-kubernetes-network/</guid>
      <description>&lt;p&gt;大家好，我是 roc，来自腾讯云容器服务(TKE)团队，经常帮助用户解决各种 K8S 的疑难杂症，积累了比较丰富的经验，本文分享几个比较复杂的网络方面的问题排查和解决思路，深入分析并展开相关知识，信息量巨大，相关经验不足的同学可能需要细细品味才能消化，我建议收藏本文反复研读，当完全看懂后我相信你的功底会更加扎实，解决问题的能力会大大提升。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文发现的问题是在使用 TKE 时遇到的，不同厂商的网络环境可能不一样，文中会对不同的问题的网络环境进行说明&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Kubernetes 踩坑分享：开启tcp_tw_recycle内核参数在NAT环境会丢包</title>
      <link>https://imroc.io/posts/lost-packets-once-enable-tcp-tw-recycle/</link>
      <pubDate>Sun, 09 Jun 2019 22:00:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/lost-packets-once-enable-tcp-tw-recycle/</guid>
      <description>原因 tcp_tw_recycle参数。它用来快速回收TIME_WAIT连接，不过如果在NAT环境下会引发问题。 RFC1323中有如下一段描述</description>
    </item>
    
    <item>
      <title>Kubernetes 最佳实践：处理内存碎片化</title>
      <link>https://imroc.io/posts/handle-memory-fragmentation/</link>
      <pubDate>Sat, 08 Jun 2019 13:59:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/handle-memory-fragmentation/</guid>
      <description>内存碎片化造成的危害 节点的内存碎片化严重，导致docker运行容器时，无法分到大的内存块，导致start docker失败。最终导致服务更新时</description>
    </item>
    
    <item>
      <title>Kubernetes 最佳实践：解决长连接服务扩容失效</title>
      <link>https://imroc.io/posts/kubernetes-scale-keepalive-service/</link>
      <pubDate>Thu, 06 Jun 2019 17:06:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-scale-keepalive-service/</guid>
      <description>在现网运营中，有很多场景为了提高效率，一般都采用建立长连接的方式来请求。我们发现在客户端以长连接请求服务端的场景下，K8S的自动扩容会失效。</description>
    </item>
    
    <item>
      <title>Kubernetes 问题定位技巧：容器内抓包</title>
      <link>https://imroc.io/posts/capture-packets-in-container/</link>
      <pubDate>Sun, 19 May 2019 11:24:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/capture-packets-in-container/</guid>
      <description>在使用 kubernetes 跑应用的时候，可能会遇到一些网络问题，比较常见的是服务端无响应(超时)或回包内容不正常，如果没找出各种配置上有问题，这时我们需要确认</description>
    </item>
    
    <item>
      <title>kubectl 高效技巧</title>
      <link>https://imroc.io/posts/efficient-kubectl/</link>
      <pubDate>Sun, 10 Mar 2019 14:05:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/efficient-kubectl/</guid>
      <description>是否有过因为使用 kubectl 经常需要重复输入命名空间而苦恼？是否觉得应该要有个记住命名空间的功能，自动记住上次使用的命名空间，不需要每次都输入？可惜没</description>
    </item>
    
    <item>
      <title>Kubernetes 泛域名动态 Service 转发解决方案</title>
      <link>https://imroc.io/posts/kubernetes-wildcard-domain-forward/</link>
      <pubDate>Sat, 22 Dec 2018 01:09:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-wildcard-domain-forward/</guid>
      <description>需求 集群对外暴露了一个公网IP作为流量入口(可以是 Ingress 或 Service)，DNS 解析配置了一个泛域名指向该IP（比如 *.test.imroc.</description>
    </item>
    
    <item>
      <title>Kubernetes 问题定位技巧：分析 ExitCode</title>
      <link>https://imroc.io/posts/kubernetes-analysis-exitcode/</link>
      <pubDate>Fri, 21 Dec 2018 16:10:00 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-analysis-exitcode/</guid>
      <description>使用 kubectl describe pod 查看异常的 pod 的状态，在容器列表里看 State 字段，其中 ExitCode 即程序退出时的状态码，正常退出时为0。如果不为0，表示异常退出，我们可以分析下原因</description>
    </item>
    
    <item>
      <title>通俗理解Kubernetes中Service、Ingress与Ingress Controller的作用与关系</title>
      <link>https://imroc.io/posts/understand-service-ingress-and-ingress-controller/</link>
      <pubDate>Tue, 24 Jul 2018 22:19:37 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/understand-service-ingress-and-ingress-controller/</guid>
      <description>通俗的讲: Service 是后端真实服务的抽象，一个 Service 可以代表多个相同的后端服务 Ingress 是反向代理规则，用来规定 HTTP/S 请求应该被转发到哪个 Service 上，比如根据请求中不同的</description>
    </item>
    
    <item>
      <title>利用Helm一键部署Kubernetes Dashboard并启用免费HTTPS</title>
      <link>https://imroc.io/posts/deploy-kubernetes-dashboard-and-enable-free-https/</link>
      <pubDate>Mon, 23 Jul 2018 21:49:54 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/deploy-kubernetes-dashboard-and-enable-free-https/</guid>
      <description>概述 Kubernetes Dashboard 是一个可以可视化查看和操作 Kubernetes 集群的一个插件 本文利用 Helm 部署它，所以请确保 Helm 已安装，安装方法参考：https://imroc.io/po</description>
    </item>
    
    <item>
      <title>利用cert-manager让Ingress启用免费的HTTPS证书</title>
      <link>https://imroc.io/posts/let-ingress-enable-free-https-with-cert-manager/</link>
      <pubDate>Mon, 23 Jul 2018 20:08:01 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/let-ingress-enable-free-https-with-cert-manager/</guid>
      <description>概述 cert-manager 是替代 kube-lego 的一个开源项目，用于在 Kubernetes 集群中自动提供 HTTPS 证书，支持 Let’s Encrypt, HashiCorp Vault 这些免费证书的签发。 本文使用 Helm 安装，所以请确保 Helm 已安装，安装</description>
    </item>
    
    <item>
      <title>使用Nginx Ingress Controller导入外部流量到Kubernetes集群内部</title>
      <link>https://imroc.io/posts/use-nginx-ingress-controller-to-expose-service/</link>
      <pubDate>Mon, 23 Jul 2018 14:29:37 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/use-nginx-ingress-controller-to-expose-service/</guid>
      <description>概述 Nginx Ingress Controller 是 Kubernetes Ingress Controller 的一种实现，作为反向代理将外部流量导入集群内部，实现将 Kubernetes 内部的 Service 暴露给外部，这样我们就能通过公网或内网直接访问集群内部的服</description>
    </item>
    
    <item>
      <title>对比Kubernetes的Nodeport、Loadbalancer和Ingress，什么时候该用哪种</title>
      <link>https://imroc.io/posts/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what/</link>
      <pubDate>Tue, 13 Mar 2018 22:45:26 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what/</guid>
      <description>本文翻译自：https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalan</description>
    </item>
    
    <item>
      <title>kubernetes源码阅读笔记：理清 kube-apiserver 的源码主线</title>
      <link>https://imroc.io/posts/kubernetes-source-code-reading-notes-kube-apiserver-code-main-line/</link>
      <pubDate>Mon, 12 Mar 2018 11:47:19 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/kubernetes-source-code-reading-notes-kube-apiserver-code-main-line/</guid>
      <description>前言 我最近开始研究 kubernetes 源码，希望将阅读笔记记录下来，分享阅读思路和心得，更好的理解 kubernetes，这是第一篇，从 kube-apiserver 开始。 开始 k8s各组件</description>
    </item>
    
    <item>
      <title>利用Katacoda免费同步Docker镜像到Docker Hub</title>
      <link>https://imroc.io/posts/sync-images-to-docker-hub-using-katacoda/</link>
      <pubDate>Fri, 09 Mar 2018 10:39:17 +0800</pubDate>
      <author>roc@imroc.io (roc)</author>
      <guid>https://imroc.io/posts/sync-images-to-docker-hub-using-katacoda/</guid>
      <description>为什么要同步 安装kubernetes的时候，我们需要用到 gcr.io/google_containers 下面的一些镜像，在国内是不能直接下载的。如果用 Self Host 方式安装，master 上的组件除</description>
    </item>
    
  </channel>
</rss>